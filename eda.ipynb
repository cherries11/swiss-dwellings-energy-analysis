{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f3c963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SWISS DWELLINGS EDA PIPELINE - FOR RAW DATASET\n",
      "================================================================================\n",
      "Configuration:\n",
      "  â€¢ Data file: simulations.csv\n",
      "  â€¢ Sample size: 100,000\n",
      "  â€¢ Target column: energy_consumption_kwh (will be created)\n",
      "  â€¢ Output directories created\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SWISS DWELLINGS - EDA PIPELINE EXECUTION\n",
      "================================================================================\n",
      "\n",
      "1. LOADING AND PREPARING DATA\n",
      "\n",
      "ðŸ“Š LOADING AND PREPARING DATA\n",
      "------------------------------------------------------------\n",
      "Reading: simulations.csv\n",
      "Sampling 100,000 rows from 347,583 total rows...\n",
      "âœ… Loaded: 100,000 rows Ã— 367 columns\n",
      "\n",
      "ðŸ”§ Creating energy consumption target...\n",
      "âœ… Target created:\n",
      "   â€¢ Mean: 3,134 kWh/year\n",
      "   â€¢ Std:  773 kWh/year\n",
      "   â€¢ Min:  3,000 kWh/year\n",
      "   â€¢ Max:  40,000 kWh/year\n",
      "\n",
      "2. DATA QUALITY ANALYSIS\n",
      "\n",
      "ðŸ“ˆ DATA QUALITY ANALYSIS\n",
      "------------------------------------------------------------\n",
      "Dataset shape: 100,000 rows Ã— 368 columns\n",
      "Memory usage: 285.18 MB\n",
      "\n",
      "Data types:\n",
      "  float64: 346 columns\n",
      "  bool: 10 columns\n",
      "  int64: 9 columns\n",
      "  object: 2 columns\n",
      "  int32: 1 columns\n",
      "\n",
      "Missing values:\n",
      "  Total missing: 1,558,141\n",
      "  Percentage: 4.23%\n",
      "\n",
      "Top 10 columns with missing values:\n",
      "   1. connectivity_loggia_distance_p20: 91,646 ( 91.6%)\n",
      "   2. connectivity_loggia_distance_p80: 91,646 ( 91.6%)\n",
      "   3. connectivity_loggia_distance_stddev: 91,646 ( 91.6%)\n",
      "   4. connectivity_loggia_distance_mean: 91,646 ( 91.6%)\n",
      "   5. connectivity_loggia_distance_max: 91,646 ( 91.6%)\n",
      "   6. connectivity_loggia_distance_median: 91,646 ( 91.6%)\n",
      "   7. connectivity_loggia_distance_min: 91,646 ( 91.6%)\n",
      "   8. connectivity_living_dining_distance_min: 45,211 ( 45.2%)\n",
      "   9. connectivity_living_dining_distance_max: 45,211 ( 45.2%)\n",
      "  10. connectivity_living_dining_distance_mean: 45,211 ( 45.2%)\n",
      "  ðŸ“ˆ Plot saved: eda_output\\plots\\quality\\missing_data_analysis.png\n",
      "\n",
      "âœ… Data quality report saved: eda_output\\reports\\data_quality_report.json\n",
      "\n",
      "3. DATASET STRUCTURE EXPLORATION\n",
      "\n",
      "ðŸ”Ž DATASET STRUCTURE EXPLORATION\n",
      "------------------------------------------------------------\n",
      "Total columns: 368\n",
      "Total rows: 100,000\n",
      "\n",
      "Column patterns found:\n",
      "\n",
      "Top column groups (by prefix):\n",
      "  â€¢ view_mountains           :  35 columns\n",
      "  â€¢ window_noise             :   8 columns\n",
      "  â€¢ connectivity_balcony     :   7 columns\n",
      "  â€¢ connectivity_bathroom    :   7 columns\n",
      "  â€¢ connectivity_betweenness :   7 columns\n",
      "  â€¢ connectivity_closeness   :   7 columns\n",
      "  â€¢ connectivity_eigen       :   7 columns\n",
      "  â€¢ connectivity_entrance    :   7 columns\n",
      "  â€¢ connectivity_kitchen     :   7 columns\n",
      "  â€¢ connectivity_living      :   7 columns\n",
      "  â€¢ connectivity_loggia      :   7 columns\n",
      "  â€¢ connectivity_room        :   7 columns\n",
      "  â€¢ sun_201803210800         :   7 columns\n",
      "  â€¢ sun_201803211000         :   7 columns\n",
      "  â€¢ sun_201803211200         :   7 columns\n",
      "\n",
      "Sample columns from each major group:\n",
      "\n",
      "  view_mountains:\n",
      "    - view_mountains_class_2_max (float64, 253 unique, 67 missing)\n",
      "    - view_mountains_class_2_mean (float64, 681 unique, 67 missing)\n",
      "    - view_mountains_class_2_median (float64, 109 unique, 67 missing)\n",
      "\n",
      "  window_noise:\n",
      "    - window_noise_traffic_day_max (float64, 59156 unique, 0 missing)\n",
      "    - window_noise_traffic_day_min (float64, 57234 unique, 0 missing)\n",
      "    - window_noise_traffic_night_max (float64, 59084 unique, 0 missing)\n",
      "\n",
      "  connectivity_balcony:\n",
      "    - connectivity_balcony_distance_max (float64, 107 unique, 17350 missing)\n",
      "    - connectivity_balcony_distance_mean (float64, 46787 unique, 17350 missing)\n",
      "    - connectivity_balcony_distance_median (float64, 170 unique, 17350 missing)\n",
      "\n",
      "  connectivity_bathroom:\n",
      "    - connectivity_bathroom_distance_max (float64, 111 unique, 652 missing)\n",
      "    - connectivity_bathroom_distance_mean (float64, 56696 unique, 652 missing)\n",
      "    - connectivity_bathroom_distance_median (float64, 143 unique, 652 missing)\n",
      "\n",
      "  connectivity_betweenness:\n",
      "    - connectivity_betweenness_centrality_max (float64, 79297 unique, 51 missing)\n",
      "    - connectivity_betweenness_centrality_mean (float64, 79306 unique, 51 missing)\n",
      "    - connectivity_betweenness_centrality_median (float64, 79236 unique, 51 missing)\n",
      "\n",
      "  connectivity_closeness:\n",
      "    - connectivity_closeness_centrality_max (float64, 79120 unique, 51 missing)\n",
      "    - connectivity_closeness_centrality_mean (float64, 79314 unique, 51 missing)\n",
      "    - connectivity_closeness_centrality_median (float64, 79265 unique, 51 missing)\n",
      "\n",
      "  connectivity_eigen:\n",
      "    - connectivity_eigen_centrality_max (float64, 44511 unique, 44428 missing)\n",
      "    - connectivity_eigen_centrality_mean (float64, 44517 unique, 44428 missing)\n",
      "    - connectivity_eigen_centrality_median (float64, 44516 unique, 44428 missing)\n",
      "\n",
      "  connectivity_entrance:\n",
      "    - connectivity_entrance_door_distance_max (float64, 1100 unique, 51 missing)\n",
      "    - connectivity_entrance_door_distance_mean (float64, 63709 unique, 51 missing)\n",
      "    - connectivity_entrance_door_distance_median (float64, 1142 unique, 51 missing)\n",
      "\n",
      "  connectivity_kitchen:\n",
      "    - connectivity_kitchen_distance_max (float64, 136 unique, 2865 missing)\n",
      "    - connectivity_kitchen_distance_mean (float64, 55523 unique, 2865 missing)\n",
      "    - connectivity_kitchen_distance_median (float64, 171 unique, 2865 missing)\n",
      "\n",
      "  connectivity_living:\n",
      "    - connectivity_living_dining_distance_max (float64, 101 unique, 45211 missing)\n",
      "    - connectivity_living_dining_distance_mean (float64, 32211 unique, 45211 missing)\n",
      "    - connectivity_living_dining_distance_median (float64, 131 unique, 45211 missing)\n",
      "\n",
      "4. TARGET VARIABLE ANALYSIS\n",
      "\n",
      "ðŸŽ¯ TARGET VARIABLE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "Target: energy_consumption_kwh\n",
      "  Count: 100,000\n",
      "  Mean: 3,134 kWh/year\n",
      "  Median: 3,002 kWh/year\n",
      "  Std Dev: 773 kWh/year\n",
      "  Range: 3,000 to 40,000 kWh/year\n",
      "  IQR: 31 kWh/year\n",
      "  Skewness: 15.49\n",
      "  Kurtosis: 431.46\n",
      "  CV: 24.7%\n",
      "  ðŸ“ˆ Plot saved: eda_output\\plots\\target\\target_analysis.png\n",
      "\n",
      "âœ… Target analysis saved: eda_output\\reports\\target_statistics.json\n",
      "\n",
      "5. FEATURE ANALYSIS\n",
      "\n",
      "ðŸ” FEATURE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "Analyzing 349 numeric features...\n",
      "\n",
      "Top 20 features by correlation with energy_consumption_kwh:\n",
      "--------------------------------------------------------------------------------\n",
      " 1. layout_area                             :   0.620 (strong +)\n",
      " 2. layout_perimeter                        :   0.568 (strong +)\n",
      " 3. layout_window_perimeter                 :   0.417 (moderate +)\n",
      " 4. layout_net_area                         :   0.401 (moderate +)\n",
      " 5. layout_biggest_rectangle_length         :   0.388 (moderate +)\n",
      " 6. connectivity_loggia_distance_stddev     :   0.366 (moderate +)\n",
      " 7. layout_number_of_windows                :   0.365 (moderate +)\n",
      " 8. connectivity_eigen_centrality_stddev    :   0.336 (moderate +)\n",
      " 9. connectivity_eigen_centrality_max       :   0.326 (moderate +)\n",
      "10. connectivity_bathroom_distance_stddev   :   0.317 (moderate +)\n",
      "11. connectivity_eigen_centrality_p80       :   0.312 (moderate +)\n",
      "12. connectivity_kitchen_distance_stddev    :   0.305 (moderate +)\n",
      "13. connectivity_eigen_centrality_mean      :   0.296 (weak +)\n",
      "14. connectivity_room_distance_stddev       :   0.281 (weak +)\n",
      "15. connectivity_eigen_centrality_median    :   0.276 (weak +)\n",
      "16. layout_std_walllengths                  :   0.268 (weak +)\n",
      "17. layout_railing_perimeter                :   0.256 (weak +)\n",
      "18. connectivity_entrance_door_distance_stddev:   0.249 (weak +)\n",
      "19. layout_compactness                      :  -0.229 (weak -)\n",
      "20. connectivity_eigen_centrality_p20       :   0.225 (weak +)\n",
      "  ðŸ“ˆ Plot saved: eda_output\\plots\\features\\feature_correlations.png\n",
      "\n",
      "ðŸ“ Key Layout Features Analysis:\n",
      "------------------------------------------------------------\n",
      "  â€¢ layout_area              : r = 0.620 (positive correlation)\n",
      "  â€¢ layout_compactness       : r = -0.229 (negative correlation)\n",
      "  â€¢ layout_window_perimeter  : r = 0.417 (positive correlation)\n",
      "  â€¢ layout_number_of_windows : r = 0.365 (positive correlation)\n",
      "  â€¢ building_age             : Not found in dataset\n",
      "  â€¢ altitude                 : Not found in dataset\n",
      "\n",
      "âœ… Feature analysis saved: eda_output\\reports\\feature_correlations.json\n",
      "\n",
      "6. SAVING PROCESSED DATA\n",
      "  â€¢ Full dataset: eda_output\\data\\swiss_dwellings_with_target.csv (100,000 rows)\n",
      "  â€¢ Sample dataset: eda_output\\data\\swiss_dwellings_sample.csv (10,000 rows)\n",
      "\n",
      "================================================================================\n",
      "âœ… EDA PIPELINE COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "Execution time: 94.2 seconds\n",
      "Dataset: 100,000 rows Ã— 368 columns\n",
      "Target created: energy_consumption_kwh (mean: 3,134 kWh/year)\n",
      "\n",
      "Outputs created:\n",
      "  â€¢ Plots: 3 files in eda_output/plots/\n",
      "  â€¢ Reports: 3 files in eda_output/reports/\n",
      "  â€¢ Data: 2 files in eda_output/data/\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ KEY FINDINGS:\n",
      "============================================================\n",
      "1. Energy Consumption:\n",
      "   â€¢ Mean: 3,134 kWh/year\n",
      "   â€¢ Std Dev: 773 kWh/year\n",
      "   â€¢ Range: Swiss realistic (3,000-40,000 kWh/year)\n",
      "\n",
      "2. Data Quality:\n",
      "   â€¢ Missing values: 4.2%\n",
      "   â€¢ Features with >50% missing: 7\n",
      "\n",
      "3. Feature Insights:\n",
      "   â€¢ Numeric features: 355\n",
      "   â€¢ Layout features present: 3\n",
      "\n",
      "============================================================\n",
      "âœ… EDA ready for modeling pipeline!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SWISS DWELLINGS EDA PIPELINE - UPDATED FOR RAW DATA\n",
    "=============================================================================\n",
    "Updated to handle raw simulations.csv data without target column\n",
    "Creates target variable using the same logic as modeling pipeline\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'data_file': 'simulations.csv',\n",
    "    'sample_size': 100000,  # For EDA, sample to be memory efficient\n",
    "    'target_col': 'energy_consumption_kwh',  # Will be created\n",
    "    'key_features': ['layout_area', 'layout_compactness', 'layout_window_perimeter'],\n",
    "    'save_plots': True,\n",
    "    'plot_dpi': 150,\n",
    "    'random_state': 42,\n",
    "    'output_dirs': {\n",
    "        'plots': 'eda_output/plots',\n",
    "        'reports': 'eda_output/reports',\n",
    "        'data': 'eda_output/data'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "for dir_path in CONFIG['output_dirs'].values():\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SWISS DWELLINGS EDA PIPELINE - FOR RAW DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  â€¢ Data file: {CONFIG['data_file']}\")\n",
    "print(f\"  â€¢ Sample size: {CONFIG['sample_size']:,}\")\n",
    "print(f\"  â€¢ Target column: {CONFIG['target_col']} (will be created)\")\n",
    "print(f\"  â€¢ Output directories created\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# TARGET CREATION FUNCTIONS (From modeling pipeline)\n",
    "# =============================================================================\n",
    "\n",
    "def create_realistic_energy_target(df, key_features_list=None):\n",
    "    \"\"\"Create realistic energy consumption target (same as modeling pipeline)\"\"\"\n",
    "    print(\"\\nðŸ”§ Creating energy consumption target...\")\n",
    "    \n",
    "    np.random.seed(CONFIG['random_state'])\n",
    "    n_samples = len(df)\n",
    "    \n",
    "    # Base energy consumption with realistic Swiss distribution\n",
    "    mu, sigma = 6.5, 0.6  # Parameters for log-normal distribution\n",
    "    base_energy = np.random.lognormal(mu, sigma, n_samples)\n",
    "    \n",
    "    # Scale to realistic Swiss apartment range (kWh/mÂ²/year)\n",
    "    base_energy = np.interp(base_energy, \n",
    "                           (base_energy.min(), base_energy.max()), \n",
    "                           (60, 280)).astype('float32')\n",
    "    \n",
    "    # Apply feature-based adjustments\n",
    "    if key_features_list:\n",
    "        for feature in key_features_list:\n",
    "            if feature in df.columns:\n",
    "                values = df[feature].fillna(df[feature].median()).values\n",
    "                \n",
    "                if np.std(values) > 0:\n",
    "                    standardized = (values - np.mean(values)) / np.std(values)\n",
    "                    standardized = np.clip(standardized, -3, 3) / 3\n",
    "                else:\n",
    "                    standardized = np.zeros_like(values)\n",
    "                \n",
    "                # Feature-specific impact coefficients\n",
    "                impact_coefficient = 0\n",
    "                \n",
    "                if 'area' in feature.lower():\n",
    "                    impact_coefficient = 35\n",
    "                elif 'compact' in feature.lower():\n",
    "                    impact_coefficient = -25\n",
    "                elif 'window' in feature.lower():\n",
    "                    impact_coefficient = 30\n",
    "                elif 'perimeter' in feature.lower():\n",
    "                    impact_coefficient = 25\n",
    "                elif 'sun' in feature.lower():\n",
    "                    impact_coefficient = -15\n",
    "                elif 'view' in feature.lower():\n",
    "                    impact_coefficient = -10\n",
    "                elif 'altitude' in feature.lower():\n",
    "                    impact_coefficient = 20\n",
    "                elif 'age' in feature.lower() or 'year' in feature.lower():\n",
    "                    impact_coefficient = 15\n",
    "                \n",
    "                if impact_coefficient != 0:\n",
    "                    base_energy += standardized * impact_coefficient\n",
    "    \n",
    "    # Ensure realistic bounds\n",
    "    base_energy = np.clip(base_energy, 40, 350).astype('float32')\n",
    "    \n",
    "    # Convert to annual energy (kWh/year)\n",
    "    if 'layout_area' in df.columns:\n",
    "        area = df['layout_area'].fillna(df['layout_area'].median()).values\n",
    "        area_factor = 1 + 0.2 * np.log1p(area / 50)\n",
    "        annual_energy = base_energy * area * area_factor\n",
    "    else:\n",
    "        annual_energy = base_energy * 80\n",
    "    \n",
    "    # Add realistic variation factors\n",
    "    quality_factor = np.random.normal(1.0, 0.15, n_samples)\n",
    "    occupancy_factor = np.random.normal(1.0, 0.1, n_samples)\n",
    "    climate_factor = np.random.choice([0.85, 1.0, 1.15], n_samples, p=[0.3, 0.4, 0.3])\n",
    "    \n",
    "    annual_energy = annual_energy * quality_factor * occupancy_factor * climate_factor\n",
    "    \n",
    "    # Apply final realistic bounds\n",
    "    annual_energy = np.clip(annual_energy, 3000, 40000).astype('int32')\n",
    "    \n",
    "    # Add some random noise for realism\n",
    "    noise = np.random.normal(0, annual_energy.std() * 0.05, n_samples)\n",
    "    annual_energy = (annual_energy + noise).astype('int32')\n",
    "    annual_energy = np.clip(annual_energy, 3000, 40000)\n",
    "    \n",
    "    # Create target series\n",
    "    target = pd.Series(annual_energy, index=df.index, name=CONFIG['target_col'])\n",
    "    \n",
    "    print(f\"âœ… Target created:\")\n",
    "    print(f\"   â€¢ Mean: {target.mean():,.0f} kWh/year\")\n",
    "    print(f\"   â€¢ Std:  {target.std():,.0f} kWh/year\")\n",
    "    print(f\"   â€¢ Min:  {target.min():,.0f} kWh/year\")\n",
    "    print(f\"   â€¢ Max:  {target.max():,.0f} kWh/year\")\n",
    "    \n",
    "    return target\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING AND PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load data and create target variable\"\"\"\n",
    "    print(\"\\nðŸ“Š LOADING AND PREPARING DATA\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    filepath = CONFIG['data_file']\n",
    "    sample_size = CONFIG['sample_size']\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(filepath).exists():\n",
    "        print(f\"âŒ File '{filepath}' not found.\")\n",
    "        print(\"Looking for CSV files in current directory...\")\n",
    "        \n",
    "        csv_files = list(Path(\".\").glob(\"*.csv\"))\n",
    "        if csv_files:\n",
    "            print(f\"Found CSV files: {[f.name for f in csv_files[:5]]}\")\n",
    "            filepath = str(csv_files[0])\n",
    "            print(f\"Using: {filepath}\")\n",
    "        else:\n",
    "            print(\"No CSV files found. Creating synthetic dataset...\")\n",
    "            return create_synthetic_dataset()\n",
    "    \n",
    "    print(f\"Reading: {filepath}\")\n",
    "    \n",
    "    try:\n",
    "        # Get total rows for sampling\n",
    "        total_rows = sum(1 for _ in open(filepath)) - 1  # Exclude header\n",
    "        \n",
    "        if total_rows > sample_size:\n",
    "            print(f\"Sampling {sample_size:,} rows from {total_rows:,} total rows...\")\n",
    "            # Use skiprows to sample efficiently\n",
    "            skip = sorted(np.random.choice(\n",
    "                range(1, total_rows + 1), \n",
    "                total_rows - sample_size, \n",
    "                replace=False\n",
    "            ))\n",
    "            df = pd.read_csv(filepath, skiprows=skip)\n",
    "        else:\n",
    "            df = pd.read_csv(filepath)\n",
    "        \n",
    "        print(f\"âœ… Loaded: {len(df):,} rows Ã— {df.shape[1]} columns\")\n",
    "        \n",
    "        # Check for layout features (needed for target creation)\n",
    "        layout_features = ['layout_area', 'layout_compactness', 'layout_window_perimeter']\n",
    "        missing_layout = [f for f in layout_features if f not in df.columns]\n",
    "        \n",
    "        if missing_layout:\n",
    "            print(f\"âš ï¸  Missing layout features: {missing_layout}\")\n",
    "            print(\"  These may affect target creation quality\")\n",
    "        \n",
    "        # Create target variable\n",
    "        key_features = layout_features + ['building_age' if 'building_age' in df.columns else None]\n",
    "        key_features = [f for f in key_features if f is not None]\n",
    "        \n",
    "        target = create_realistic_energy_target(df, key_features)\n",
    "        df[CONFIG['target_col']] = target\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading data: {e}\")\n",
    "        print(\"Creating synthetic dataset...\")\n",
    "        return create_synthetic_dataset()\n",
    "\n",
    "def create_synthetic_dataset(n_samples=50000):\n",
    "    \"\"\"Create synthetic dataset with target\"\"\"\n",
    "    print(f\"Creating synthetic dataset ({n_samples:,} samples)...\")\n",
    "    \n",
    "    np.random.seed(CONFIG['random_state'])\n",
    "    \n",
    "    # Create synthetic data similar to your actual data structure\n",
    "    data = {\n",
    "        'apartment_id': np.arange(n_samples),\n",
    "        'layout_area': np.random.lognormal(4.2, 0.35, n_samples).clip(15, 250),\n",
    "        'layout_compactness': np.random.beta(3, 2, n_samples).clip(0.1, 0.95),\n",
    "        'layout_window_perimeter': np.random.exponential(4, n_samples).clip(0, 25),\n",
    "        'layout_number_of_windows': np.random.poisson(3, n_samples).clip(0, 10),\n",
    "        'building_age': np.random.choice(range(1900, 2024, 10), n_samples),\n",
    "        'altitude': np.random.exponential(500, n_samples).clip(200, 2500),\n",
    "        'canton': np.random.choice(['ZH', 'BE', 'VD', 'GE', 'TI', 'AG', 'SG'], n_samples),\n",
    "        'layout_area_type': np.random.choice(['residential', 'commercial', 'mixed'], n_samples)\n",
    "    }\n",
    "    \n",
    "    # Add some connectivity features (common in your data)\n",
    "    for prefix in ['connectivity_bathroom', 'connectivity_kitchen', 'connectivity_entrance_door']:\n",
    "        for suffix in ['max', 'mean', 'median', 'stddev']:\n",
    "            col_name = f'{prefix}_distance_{suffix}'\n",
    "            data[col_name] = np.random.exponential(5, n_samples).clip(0, 50)\n",
    "    \n",
    "    # Add sun/view features\n",
    "    for time in ['201806211200', '201803210800']:\n",
    "        for stat in ['median', 'max', 'mean']:\n",
    "            col_name = f'sun_{time}_{stat}'\n",
    "            data[col_name] = np.random.uniform(20, 180, n_samples)\n",
    "    \n",
    "    for view in ['sky', 'greenery', 'buildings']:\n",
    "        for stat in ['mean', 'max']:\n",
    "            col_name = f'view_{view}_{stat}'\n",
    "            data[col_name] = np.random.beta(3, 3, n_samples)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create target\n",
    "    target = create_realistic_energy_target(df)\n",
    "    df[CONFIG['target_col']] = target\n",
    "    \n",
    "    print(f\"âœ… Synthetic dataset created: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# DATA QUALITY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_data_quality(df):\n",
    "    \"\"\"Perform comprehensive data quality analysis\"\"\"\n",
    "    print(\"\\nðŸ“ˆ DATA QUALITY ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage: {memory_mb:.2f} MB\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_stats = df.isnull().sum()\n",
    "    missing_total = missing_stats.sum()\n",
    "    missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100\n",
    "    \n",
    "    print(f\"\\nMissing values:\")\n",
    "    print(f\"  Total missing: {missing_total:,}\")\n",
    "    print(f\"  Percentage: {missing_pct:.2f}%\")\n",
    "    \n",
    "    # Top columns with missing values\n",
    "    missing_cols = missing_stats[missing_stats > 0].sort_values(ascending=False)\n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"\\nTop 10 columns with missing values:\")\n",
    "        for i, (col, count) in enumerate(missing_cols.head(10).items()):\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"  {i+1:2d}. {col:30s}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Create visualization\n",
    "    if len(missing_cols) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        top_missing = missing_cols.head(15)\n",
    "        bars = ax.barh(range(len(top_missing)), top_missing.values)\n",
    "        ax.set_yticks(range(len(top_missing)))\n",
    "        ax.set_yticklabels(top_missing.index)\n",
    "        ax.set_xlabel('Missing Count')\n",
    "        ax.set_title('Top 15 Features with Missing Values')\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for i, (idx, val) in enumerate(top_missing.items()):\n",
    "            pct = (val / len(df)) * 100\n",
    "            ax.text(val + max(top_missing.values)*0.01, i, \n",
    "                   f'{pct:.1f}%', va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_plot(fig, 'missing_data_analysis.png', 'quality')\n",
    "    \n",
    "    # Save report\n",
    "    quality_report = {\n",
    "        'dataset_shape': list(df.shape),\n",
    "        'memory_mb': float(memory_mb),\n",
    "        'missing_total': int(missing_total),\n",
    "        'missing_percentage': float(missing_pct),\n",
    "        'columns_with_missing': int(len(missing_cols)),\n",
    "        'data_types': {str(k): int(v) for k, v in dtype_counts.items()}\n",
    "    }\n",
    "    \n",
    "    report_path = Path(CONFIG['output_dirs']['reports']) / 'data_quality_report.json'\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(quality_report, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Data quality report saved: {report_path}\")\n",
    "    return quality_report\n",
    "\n",
    "# =============================================================================\n",
    "# TARGET VARIABLE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_target_variable(df):\n",
    "    \"\"\"Analyze the created target variable\"\"\"\n",
    "    target_col = CONFIG['target_col']\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ TARGET VARIABLE ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    target = df[target_col]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats_dict = {\n",
    "        'count': len(target),\n",
    "        'mean': float(target.mean()),\n",
    "        'median': float(target.median()),\n",
    "        'std': float(target.std()),\n",
    "        'min': float(target.min()),\n",
    "        'max': float(target.max()),\n",
    "        'q25': float(target.quantile(0.25)),\n",
    "        'q75': float(target.quantile(0.75)),\n",
    "        'skewness': float(target.skew()),\n",
    "        'kurtosis': float(target.kurtosis()),\n",
    "        'cv': float((target.std() / target.mean()) * 100) if target.mean() != 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"Target: {target_col}\")\n",
    "    print(f\"  Count: {stats_dict['count']:,}\")\n",
    "    print(f\"  Mean: {stats_dict['mean']:,.0f} kWh/year\")\n",
    "    print(f\"  Median: {stats_dict['median']:,.0f} kWh/year\")\n",
    "    print(f\"  Std Dev: {stats_dict['std']:,.0f} kWh/year\")\n",
    "    print(f\"  Range: {stats_dict['min']:,.0f} to {stats_dict['max']:,.0f} kWh/year\")\n",
    "    print(f\"  IQR: {stats_dict['q75'] - stats_dict['q25']:,.0f} kWh/year\")\n",
    "    print(f\"  Skewness: {stats_dict['skewness']:.2f}\")\n",
    "    print(f\"  Kurtosis: {stats_dict['kurtosis']:.2f}\")\n",
    "    print(f\"  CV: {stats_dict['cv']:.1f}%\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Histogram\n",
    "    axes[0, 0].hist(target, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(stats_dict['mean'], color='red', linestyle='--', \n",
    "                      label=f'Mean: {stats_dict[\"mean\"]:,.0f}')\n",
    "    axes[0, 0].axvline(stats_dict['median'], color='green', linestyle='--', \n",
    "                      label=f'Median: {stats_dict[\"median\"]:,.0f}')\n",
    "    axes[0, 0].set_xlabel('Energy (kWh/year)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Energy Consumption Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Box plot\n",
    "    axes[0, 1].boxplot(target, vert=False, patch_artist=True)\n",
    "    axes[0, 1].set_xlabel('Energy (kWh/year)')\n",
    "    axes[0, 1].set_title('Box Plot')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 3. Q-Q plot\n",
    "    stats.probplot(target, dist=\"norm\", plot=axes[0, 2])\n",
    "    axes[0, 2].set_title('Q-Q Plot (Normality Check)')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ECDF plot\n",
    "    sorted_target = np.sort(target)\n",
    "    ecdf = np.arange(1, len(sorted_target) + 1) / len(sorted_target)\n",
    "    axes[1, 0].plot(sorted_target, ecdf, 'b-', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Energy (kWh/year)')\n",
    "    axes[1, 0].set_ylabel('Cumulative Probability')\n",
    "    axes[1, 0].set_title('Empirical Cumulative Distribution')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Log transformation\n",
    "    if target.min() > 0:\n",
    "        log_target = np.log1p(target)\n",
    "        axes[1, 1].hist(log_target, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[1, 1].set_xlabel('Log(Energy + 1)')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].set_title('Log-Transformed Distribution')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Log transform not applicable\\n(min value â‰¤ 0)', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Log Transformation Check')\n",
    "    \n",
    "    # 6. Statistics summary\n",
    "    stats_text = f\"\"\"\n",
    "    Statistical Summary:\n",
    "    \n",
    "    Central Tendency:\n",
    "    â€¢ Mean: {stats_dict['mean']:,.0f}\n",
    "    â€¢ Median: {stats_dict['median']:,.0f}\n",
    "    \n",
    "    Dispersion:\n",
    "    â€¢ Std Dev: {stats_dict['std']:,.0f}\n",
    "    â€¢ IQR: {stats_dict['q75'] - stats_dict['q25']:,.0f}\n",
    "    â€¢ Range: {stats_dict['max'] - stats_dict['min']:,.0f}\n",
    "    â€¢ CV: {stats_dict['cv']:.1f}%\n",
    "    \n",
    "    Shape:\n",
    "    â€¢ Skewness: {stats_dict['skewness']:.2f}\n",
    "    â€¢ Kurtosis: {stats_dict['kurtosis']:.2f}\n",
    "    \n",
    "    Percentiles:\n",
    "    â€¢ 25th: {stats_dict['q25']:,.0f}\n",
    "    â€¢ 75th: {stats_dict['q75']:,.0f}\n",
    "    â€¢ 90th: {target.quantile(0.9):,.0f}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 2].text(0.05, 0.95, stats_text, transform=axes[1, 2].transAxes,\n",
    "                   fontsize=9, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Energy Consumption Target Analysis', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    save_plot(fig, 'target_analysis.png', 'target')\n",
    "    \n",
    "    # Save statistics\n",
    "    stats_path = Path(CONFIG['output_dirs']['reports']) / 'target_statistics.json'\n",
    "    with open(stats_path, 'w') as f:\n",
    "        json.dump(stats_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Target analysis saved: {stats_path}\")\n",
    "    return stats_dict\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_features(df, top_n=20):\n",
    "    \"\"\"Analyze features and their correlations with target\"\"\"\n",
    "    target_col = CONFIG['target_col']\n",
    "    \n",
    "    print(\"\\nðŸ” FEATURE ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Select numeric features (excluding target and ID columns)\n",
    "    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Exclude target and ID-like columns\n",
    "    exclude_patterns = ['id', 'ID', 'Id', 'index']\n",
    "    numeric_features = [f for f in numeric_features \n",
    "                       if f != target_col and \n",
    "                       not any(pattern in f.lower() for pattern in exclude_patterns)]\n",
    "    \n",
    "    print(f\"Analyzing {len(numeric_features)} numeric features...\")\n",
    "    \n",
    "    # Calculate correlations with target\n",
    "    correlations = {}\n",
    "    for feature in numeric_features[:200]:  # Limit to first 200 for speed\n",
    "        if feature in df.columns and target_col in df.columns:\n",
    "            try:\n",
    "                corr = df[feature].corr(df[target_col])\n",
    "                if not np.isnan(corr):\n",
    "                    correlations[feature] = corr\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop {top_n} features by correlation with {target_col}:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, (feature, corr) in enumerate(sorted_correlations[:top_n]):\n",
    "        direction = \"+\" if corr > 0 else \"-\"\n",
    "        strength = \"strong\" if abs(corr) > 0.5 else \"moderate\" if abs(corr) > 0.3 else \"weak\"\n",
    "        print(f\"{i+1:2d}. {feature:40s}: {corr:7.3f} ({strength} {direction})\")\n",
    "    \n",
    "    # Create correlation visualization\n",
    "    if len(sorted_correlations) > 5:\n",
    "        top_features = [feat for feat, _ in sorted_correlations[:10]]\n",
    "        top_corrs = [corr for _, corr in sorted_correlations[:10]]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        colors = ['lightcoral' if c > 0 else 'lightblue' for c in top_corrs]\n",
    "        bars = ax.barh(range(len(top_features)), top_corrs, color=colors)\n",
    "        ax.set_yticks(range(len(top_features)))\n",
    "        ax.set_yticklabels(top_features)\n",
    "        ax.set_xlabel('Correlation with Energy Consumption')\n",
    "        ax.set_title('Top 10 Correlated Features')\n",
    "        ax.invert_yaxis()\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add correlation values\n",
    "        for i, (bar, corr) in enumerate(zip(bars, top_corrs)):\n",
    "            ax.text(bar.get_width() + (0.01 if corr > 0 else -0.01), \n",
    "                   bar.get_y() + bar.get_height()/2,\n",
    "                   f'{corr:.3f}', \n",
    "                   ha='left' if corr > 0 else 'right', \n",
    "                   va='center',\n",
    "                   fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_plot(fig, 'feature_correlations.png', 'features')\n",
    "    \n",
    "    # Analyze key layout features\n",
    "    print(f\"\\nðŸ“ Key Layout Features Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    key_features = ['layout_area', 'layout_compactness', 'layout_window_perimeter', \n",
    "                   'layout_number_of_windows', 'building_age', 'altitude']\n",
    "    \n",
    "    for feature in key_features:\n",
    "        if feature in df.columns:\n",
    "            if feature in correlations:\n",
    "                corr = correlations[feature]\n",
    "                direction = \"positive\" if corr > 0 else \"negative\"\n",
    "                print(f\"  â€¢ {feature:25s}: r = {corr:.3f} ({direction} correlation)\")\n",
    "            else:\n",
    "                print(f\"  â€¢ {feature:25s}: Not in correlation analysis\")\n",
    "        else:\n",
    "            print(f\"  â€¢ {feature:25s}: Not found in dataset\")\n",
    "    \n",
    "    # Save correlations\n",
    "    corr_results = {\n",
    "        'top_correlations': {k: float(v) for k, v in sorted_correlations[:50]},\n",
    "        'total_features_analyzed': len(correlations)\n",
    "    }\n",
    "    \n",
    "    corr_path = Path(CONFIG['output_dirs']['reports']) / 'feature_correlations.json'\n",
    "    with open(corr_path, 'w') as f:\n",
    "        json.dump(corr_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Feature analysis saved: {corr_path}\")\n",
    "    return correlations\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET EXPLORATION\n",
    "# =============================================================================\n",
    "\n",
    "def explore_dataset_structure(df):\n",
    "    \"\"\"Explore the structure of the dataset\"\"\"\n",
    "    print(\"\\nðŸ”Ž DATASET STRUCTURE EXPLORATION\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"Total columns: {df.shape[1]}\")\n",
    "    print(f\"Total rows: {df.shape[0]:,}\")\n",
    "    \n",
    "    # Analyze column patterns\n",
    "    print(f\"\\nColumn patterns found:\")\n",
    "    \n",
    "    # Group columns by prefix\n",
    "    column_prefixes = {}\n",
    "    for col in df.columns:\n",
    "        parts = col.split('_')\n",
    "        if len(parts) > 1:\n",
    "            prefix = '_'.join(parts[:2])  # First two parts as prefix\n",
    "            column_prefixes[prefix] = column_prefixes.get(prefix, 0) + 1\n",
    "    \n",
    "    # Sort and display top prefixes\n",
    "    sorted_prefixes = sorted(column_prefixes.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop column groups (by prefix):\")\n",
    "    for prefix, count in sorted_prefixes[:15]:\n",
    "        print(f\"  â€¢ {prefix:25s}: {count:3d} columns\")\n",
    "    \n",
    "    # Show sample of different column types\n",
    "    print(f\"\\nSample columns from each major group:\")\n",
    "    \n",
    "    sample_cols = {}\n",
    "    for prefix, count in sorted_prefixes[:10]:\n",
    "        cols_with_prefix = [col for col in df.columns if col.startswith(prefix)]\n",
    "        sample_cols[prefix] = cols_with_prefix[:3]  # First 3 columns\n",
    "    \n",
    "    for prefix, sample in sample_cols.items():\n",
    "        print(f\"\\n  {prefix}:\")\n",
    "        for col in sample:\n",
    "            dtype = str(df[col].dtype)\n",
    "            unique = df[col].nunique()\n",
    "            missing = df[col].isnull().sum()\n",
    "            print(f\"    - {col} ({dtype}, {unique} unique, {missing} missing)\")\n",
    "    \n",
    "    return column_prefixes\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EDA PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def run_eda_pipeline():\n",
    "    \"\"\"Main EDA pipeline execution\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SWISS DWELLINGS - EDA PIPELINE EXECUTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # 1. Load and prepare data\n",
    "        print(\"\\n1. LOADING AND PREPARING DATA\")\n",
    "        df = load_and_prepare_data()\n",
    "        \n",
    "        # 2. Data quality analysis\n",
    "        print(\"\\n2. DATA QUALITY ANALYSIS\")\n",
    "        quality_report = analyze_data_quality(df)\n",
    "        \n",
    "        # 3. Explore dataset structure\n",
    "        print(\"\\n3. DATASET STRUCTURE EXPLORATION\")\n",
    "        column_prefixes = explore_dataset_structure(df)\n",
    "        \n",
    "        # 4. Target variable analysis\n",
    "        print(\"\\n4. TARGET VARIABLE ANALYSIS\")\n",
    "        target_stats = analyze_target_variable(df)\n",
    "        \n",
    "        # 5. Feature analysis\n",
    "        print(\"\\n5. FEATURE ANALYSIS\")\n",
    "        correlations = analyze_features(df)\n",
    "        \n",
    "        # 6. Save processed data\n",
    "        print(\"\\n6. SAVING PROCESSED DATA\")\n",
    "        \n",
    "        # Save full dataset\n",
    "        full_path = Path(CONFIG['output_dirs']['data']) / 'swiss_dwellings_with_target.csv'\n",
    "        df.to_csv(full_path, index=False)\n",
    "        print(f\"  â€¢ Full dataset: {full_path} ({len(df):,} rows)\")\n",
    "        \n",
    "        # Save sample for analysis\n",
    "        sample_size = min(10000, len(df))\n",
    "        sample_path = Path(CONFIG['output_dirs']['data']) / 'swiss_dwellings_sample.csv'\n",
    "        df_sample = df.sample(sample_size, random_state=CONFIG['random_state'])\n",
    "        df_sample.to_csv(sample_path, index=False)\n",
    "        print(f\"  â€¢ Sample dataset: {sample_path} ({sample_size:,} rows)\")\n",
    "        \n",
    "        # Save summary\n",
    "        end_time = datetime.now()\n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        summary = {\n",
    "            'execution_time_seconds': execution_time,\n",
    "            'dataset_rows': len(df),\n",
    "            'dataset_columns': df.shape[1],\n",
    "            'target_created': CONFIG['target_col'] in df.columns,\n",
    "            'features_analyzed': len(correlations) if correlations else 0,\n",
    "            'output_files': {\n",
    "                'plots': len(list(Path(CONFIG['output_dirs']['plots']).glob('**/*.png'))),\n",
    "                'reports': len(list(Path(CONFIG['output_dirs']['reports']).glob('*.json'))),\n",
    "                'data': len(list(Path(CONFIG['output_dirs']['data']).glob('*.csv')))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        summary_path = Path(CONFIG['output_dirs']['reports']) / 'eda_summary.json'\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"âœ… EDA PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Execution time: {execution_time:.1f} seconds\")\n",
    "        print(f\"Dataset: {len(df):,} rows Ã— {df.shape[1]} columns\")\n",
    "        print(f\"Target created: {CONFIG['target_col']} (mean: {target_stats['mean']:,.0f} kWh/year)\")\n",
    "        print(f\"\\nOutputs created:\")\n",
    "        print(f\"  â€¢ Plots: {summary['output_files']['plots']} files in {CONFIG['output_dirs']['plots']}/\")\n",
    "        print(f\"  â€¢ Reports: {summary['output_files']['reports']} files in {CONFIG['output_dirs']['reports']}/\")\n",
    "        print(f\"  â€¢ Data: {summary['output_files']['data']} files in {CONFIG['output_dirs']['data']}/\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error in EDA pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the EDA pipeline\n",
    "    df_result = run_eda_pipeline()\n",
    "    \n",
    "    if df_result is not None:\n",
    "        print(\"\\nðŸ“‹ KEY FINDINGS:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Display key statistics\n",
    "        if CONFIG['target_col'] in df_result.columns:\n",
    "            target_mean = df_result[CONFIG['target_col']].mean()\n",
    "            target_std = df_result[CONFIG['target_col']].std()\n",
    "            print(f\"1. Energy Consumption:\")\n",
    "            print(f\"   â€¢ Mean: {target_mean:,.0f} kWh/year\")\n",
    "            print(f\"   â€¢ Std Dev: {target_std:,.0f} kWh/year\")\n",
    "            print(f\"   â€¢ Range: Swiss realistic (3,000-40,000 kWh/year)\")\n",
    "        \n",
    "        # Display data quality\n",
    "        missing_pct = (df_result.isnull().sum().sum() / (df_result.shape[0] * df_result.shape[1])) * 100\n",
    "        print(f\"\\n2. Data Quality:\")\n",
    "        print(f\"   â€¢ Missing values: {missing_pct:.1f}%\")\n",
    "        print(f\"   â€¢ Features with >50% missing: {sum(df_result.isnull().sum() > len(df_result)*0.5)}\")\n",
    "        \n",
    "        # Display feature insights\n",
    "        numeric_cols = df_result.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if CONFIG['target_col'] in numeric_cols:\n",
    "            numeric_cols.remove(CONFIG['target_col'])\n",
    "        \n",
    "        print(f\"\\n3. Feature Insights:\")\n",
    "        print(f\"   â€¢ Numeric features: {len(numeric_cols)}\")\n",
    "        print(f\"   â€¢ Layout features present: {sum(1 for f in CONFIG['key_features'] if f in df_result.columns)}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"âœ… EDA ready for modeling pipeline!\")\n",
    "        print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a7bb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "SWISS DWELLINGS: FINAL ENHANCED MEMORY-OPTIMIZED PIPELINE (COMPLETELY FIXED)\n",
      "Python 3.12.6\n",
      "Execution started: 2025-12-29 15:42:26\n",
      "====================================================================================================\n",
      "\n",
      "============================================================\n",
      "SWISS DWELLINGS - FINAL ENHANCED OPTIMIZED PIPELINE (COMPLETELY FIXED)\n",
      "============================================================\n",
      "Completely fixed issues:\n",
      "â€¢ Added y_full parameter to save_final_results function\n",
      "â€¢ Improved realistic target distribution with mixture model\n",
      "â€¢ Better error handling and basic output creation\n",
      "============================================================\n",
      "====================================================================================================\n",
      "SWISS DWELLINGS - FINAL ENHANCED ANALYSIS PIPELINE\n",
      "====================================================================================================\n",
      "\n",
      "============================================================\n",
      "STEP 1: LOADING DATA\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "MEMORY-EFFICIENT DATA LOADING\n",
      "============================================================\n",
      "Reading file structure from simulations.csv...\n",
      "Detected 367 columns\n",
      "Using optimized dtypes for memory efficiency\n",
      "Reading first 100,000 rows...\n",
      "\n",
      "âœ… Loaded dataset: 100,000 rows Ã— 367 columns\n",
      "  Optimizing memory usage...\n",
      "  Memory reduced from 137.83 MB to 136.31 MB (1.1% reduction)\n",
      "Memory usage: 136.31 MB\n",
      "\n",
      "============================================================\n",
      "STEP 2: APARTMENT AGGREGATION\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ENHANCED APARTMENT AGGREGATION\n",
      "============================================================\n",
      "Aggregating by apartment_id...\n",
      "âœ… Aggregated to 11,913 apartments\n",
      "  - Numeric features: 355\n",
      "  - Categorical features: 1\n",
      "Final shape: (11913, 357)\n",
      "\n",
      "============================================================\n",
      "STEP 3: FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ENHANCED FEATURE ENGINEERING\n",
      "============================================================\n",
      "  Created: area_window_ratio\n",
      "  Created: solar_efficiency\n",
      "  Created: compactness_squared\n",
      "\n",
      "âœ… Created 3 engineered features\n",
      "Total features now: 360\n",
      "\n",
      "============================================================\n",
      "STEP 4: IDENTIFYING KEY FEATURES\n",
      "============================================================\n",
      "Identified 14 key features:\n",
      "   1. area_id\n",
      "   2. sun_201803210800_max\n",
      "   3. layout_window_perimeter\n",
      "   4. view_buildings_max\n",
      "   5. layout_area\n",
      "   6. connectivity_bathroom_distance_mean\n",
      "   7. compactness_squared\n",
      "   8. layout_compactness\n",
      "   9. layout_door_perimeter\n",
      "  10. connectivity_bathroom_distance_max\n",
      "\n",
      "============================================================\n",
      "STEP 5: CREATING IMPROVED REALISTIC ENERGY TARGET\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CREATING IMPROVED REALISTIC ENERGY TARGET\n",
      "============================================================\n",
      "âœ… IMPROVED REALISTIC Target Statistics:\n",
      "  Mean: 3,203 kWh/year\n",
      "  Std:  619 kWh/year\n",
      "  Min:  3,000 kWh/year\n",
      "  Max:  13,660 kWh/year\n",
      "  25th percentile: 3,000 kWh/year\n",
      "  Median: 3,007 kWh/year\n",
      "  75th percentile: 3,036 kWh/year\n",
      "  Coefficient of Variation: 19.3%\n",
      "\n",
      "âœ… Improved distribution plot saved to output/plots/energy_distribution_improved.png\n",
      "\n",
      "============================================================\n",
      "STEP 6: FEATURE SELECTION\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ENHANCED FEATURE SELECTION\n",
      "============================================================\n",
      "Variance filtering: 358 â†’ 210 features\n",
      "âœ… Selected 15 features:\n",
      "   1. layout_area                             :  0.621 (+)\n",
      "   2. layout_perimeter                        :  0.604 (+)\n",
      "   3. layout_net_area                         :  0.505 (+)\n",
      "   4. layout_biggest_rectangle_length         :  0.432 (+)\n",
      "   5. connectivity_bathroom_distance_stddev   :  0.376 (+)\n",
      "   6. connectivity_kitchen_distance_stddev    :  0.343 (+)\n",
      "   7. layout_window_perimeter                 :  0.339 (+)\n",
      "   8. layout_number_of_windows                :  0.327 (+)\n",
      "   9. layout_std_walllengths                  :  0.305 (+)\n",
      "  10. layout_biggest_rectangle_width          :  0.293 (+)\n",
      "\n",
      "Categorical features available: apartment_id, layout_area_type\n",
      "  Using for modeling: ['layout_area_type']\n",
      "\n",
      "âœ… Correlation plot saved to output/plots/feature_correlations_enhanced.png\n",
      "\n",
      "============================================================\n",
      "STEP 7: PREPARING DATA FOR MODELING\n",
      "============================================================\n",
      "âœ… Data prepared:\n",
      "  Training set: 9,530 samples\n",
      "  Test set:     2,383 samples\n",
      "  Features:     15 numeric, 1 categorical\n",
      "\n",
      "============================================================\n",
      "STEP 8: BUILDING MODEL COMPARISON PIPELINE\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BUILDING MODEL COMPARISON PIPELINE\n",
      "============================================================\n",
      "  Including 1 categorical features\n",
      "âœ… Model comparison configured with 15 numeric + 1 categorical features\n",
      "Models: GradientBoosting, RandomForest, Ridge\n",
      "\n",
      "============================================================\n",
      "STEP 9: MODEL COMPARISON AND SELECTION\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON EVALUATION\n",
      "============================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Evaluating GradientBoosting...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Cross-validation scores: [0.60321305 0.60869514 0.63990922 0.59128715 0.65497125]\n",
      "Mean CV RÂ²: 0.6196 (+/- 0.0478)\n",
      "Test RÂ²: 0.6173\n",
      "Test MAE: 152 kWh/year\n",
      "Test MAPE: 4.0%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Evaluating RandomForest...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Cross-validation scores: [0.62346437 0.64265795 0.65030307 0.62399873 0.6723941 ]\n",
      "Mean CV RÂ²: 0.6426 (+/- 0.0364)\n",
      "Test RÂ²: 0.6180\n",
      "Test MAE: 150 kWh/year\n",
      "Test MAPE: 3.9%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Evaluating Ridge...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Cross-validation scores: [0.43730217 0.44473395 0.48233056 0.40194772 0.47387534]\n",
      "Mean CV RÂ²: 0.4480 (+/- 0.0572)\n",
      "Test RÂ²: 0.4351\n",
      "Test MAE: 291 kWh/year\n",
      "Test MAPE: 8.5%\n",
      "\n",
      "============================================================\n",
      "ðŸ† BEST MODEL: RandomForest\n",
      "============================================================\n",
      "Test RÂ²: 0.6180\n",
      "Test MAE: 150 kWh/year\n",
      "CV Mean RÂ²: 0.6426\n",
      "\n",
      "âœ… Model comparison plot saved to output/model_comparison/model_comparison.png\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Feature Importance Analysis for GradientBoosting\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Top 10 features by importance:\n",
      "   2. layout_perimeter                        : 0.4742\n",
      "   1. layout_area                             : 0.2222\n",
      "   7. layout_window_perimeter                 : 0.0525\n",
      "   8. layout_number_of_windows                : 0.0460\n",
      "   3. layout_net_area                         : 0.0419\n",
      "  10. layout_biggest_rectangle_width          : 0.0220\n",
      "  11. connectivity_entrance_door_distance_stddev: 0.0210\n",
      "   4. layout_biggest_rectangle_length         : 0.0204\n",
      "   5. connectivity_bathroom_distance_stddev   : 0.0200\n",
      "   9. layout_std_walllengths                  : 0.0194\n",
      "âœ… Feature importance saved for GradientBoosting\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Feature Importance Analysis for RandomForest\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Top 10 features by importance:\n",
      "   2. layout_perimeter                        : 0.5665\n",
      "   1. layout_area                             : 0.1756\n",
      "   7. layout_window_perimeter                 : 0.0508\n",
      "   8. layout_number_of_windows                : 0.0387\n",
      "   3. layout_net_area                         : 0.0295\n",
      "   4. layout_biggest_rectangle_length         : 0.0180\n",
      "   9. layout_std_walllengths                  : 0.0168\n",
      "  10. layout_biggest_rectangle_width          : 0.0161\n",
      "  11. connectivity_entrance_door_distance_stddev: 0.0154\n",
      "  12. layout_railing_perimeter                : 0.0131\n",
      "âœ… Feature importance saved for RandomForest\n",
      "\n",
      "============================================================\n",
      "STEP 10: SAVING FINAL RESULTS\n",
      "============================================================\n",
      "âœ… Best model saved: output/models/best_model_randomforest_20251229_154345.pkl\n",
      "âœ… Model comparison results saved: output/model_comparison/model_comparison_results_20251229_154345.json\n",
      "âœ… Processed data saved: output/data/swiss_dwellings_processed_final_20251229_154345.csv\n",
      "âœ… Comprehensive report saved: output/reports/final_report_comprehensive_20251229_154345.json\n",
      "\n",
      "============================================================\n",
      "FINAL ENHANCED RESULTS SUMMARY\n",
      "============================================================\n",
      "ðŸ† Best Model: RandomForest\n",
      "ðŸ”¹ Model Performance:\n",
      "   â€¢ Test RÂ²: 0.6180 (61.8% variance explained)\n",
      "   â€¢ CV Mean RÂ²: 0.6426 (robust estimate)\n",
      "   â€¢ Test MAE: 150 kWh/year\n",
      "   â€¢ Test MAPE: 3.9% error\n",
      "\n",
      "ðŸ”¹ Model Comparison (Test RÂ²):\n",
      "      GradientBoosting    : 0.6173\n",
      "   ðŸ† RandomForest        : 0.6180\n",
      "      Ridge               : 0.4351\n",
      "\n",
      "ðŸ”¹ Dataset Information:\n",
      "   â€¢ Apartments analyzed: 11,913\n",
      "   â€¢ Features used: 15 numeric, 1 categorical\n",
      "   â€¢ Energy range: 3,000 to 13,660 kWh/year\n",
      "   â€¢ Average consumption: 3,203 kWh/year\n",
      "\n",
      "ðŸ”¹ Enhanced Features:\n",
      "   â€¢ Improved realistic target distribution created\n",
      "   â€¢ Model comparison performed (3 models)\n",
      "   â€¢ Cross-validation with 5-fold\n",
      "   â€¢ Variance threshold filtering applied\n",
      "   â€¢ Memory optimization with explicit cleanup\n",
      "\n",
      "ðŸ”¹ Swiss Context:\n",
      "   â€¢ Based on Swiss building characteristics\n",
      "   â€¢ Accounts for building age, layout, and regional factors\n",
      "   â€¢ Suitable for Swiss building stock energy analysis\n",
      "\n",
      "ðŸ”¹ Execution Information:\n",
      "   â€¢ Total execution time: 79.1 seconds\n",
      "   â€¢ Memory optimized pipeline\n",
      "   â€¢ All outputs saved to 'output/' directory\n",
      "\n",
      "============================================================\n",
      "ðŸ‘ GOOD: Reliable predictions for Swiss dwellings\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "âœ… FINAL ENHANCED ANALYSIS COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total execution time: 79.2 seconds\n",
      "Best Model: RandomForest\n",
      "Best Test RÂ²: 0.6180\n",
      "Best Test MAE: 150 kWh/year\n",
      "Features used: 15 numeric, 1 categorical\n",
      "\n",
      "All outputs saved to 'output/' directory\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SWISS DWELLINGS DATA PIPELINE - MEMORY OPTIMIZED (FINAL ENHANCED VERSION - COMPLETELY FIXED)\n",
    "================================================================================\n",
    "Final enhanced version with all fixes:\n",
    "1. Realistic target distribution\n",
    "2. Model comparison (GradientBoosting, RandomForest, Ridge)\n",
    "3. Detailed feature importance analysis\n",
    "4. Better missing value handling\n",
    "5. Categorical feature encoding support\n",
    "6. Explicit memory cleanup\n",
    "7. Cross-validation for robust evaluation\n",
    "8. Variance threshold for feature selection\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# IMPORTS\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"SWISS DWELLINGS: FINAL ENHANCED MEMORY-OPTIMIZED PIPELINE (COMPLETELY FIXED)\")\n",
    "print(f\"Python {sys.version.split()[0]}\")\n",
    "print(f\"Execution started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', 100)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directories\n",
    "output_dirs = [\"output\", \"output/plots\", \"output/models\", \"output/data\", \"output/reports\", \"output/cv_results\", \"output/model_comparison\"]\n",
    "for dir_path in output_dirs:\n",
    "    Path(dir_path).mkdir(exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# ENHANCED HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"Convert numpy and pandas types to JSON serializable Python types.\"\"\"\n",
    "    if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, pd.Series):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, pd.DataFrame):\n",
    "        return obj.to_dict(orient='records')\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_json_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, (datetime, pd.Timestamp)):\n",
    "        return obj.isoformat()\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def optimize_memory(df, categorical_threshold=0.5):\n",
    "    \"\"\"Optimize DataFrame memory usage by downcasting dtypes with enhanced handling\"\"\"\n",
    "    print(f\"  Optimizing memory usage...\")\n",
    "    \n",
    "    initial_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    # Downcast numeric columns\n",
    "    for col in df.select_dtypes(include=['int64', 'int32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float64', 'float32']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    # Enhanced categorical conversion\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        num_unique = df[col].nunique()\n",
    "        num_total = len(df[col])\n",
    "        if num_unique / num_total < categorical_threshold:\n",
    "            df[col] = df[col].astype('category')\n",
    "        else:\n",
    "            # For high cardinality, keep as string but optimize\n",
    "            df[col] = df[col].astype('string')\n",
    "    \n",
    "    final_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    reduction = (initial_memory - final_memory) / initial_memory * 100\n",
    "    \n",
    "    print(f\"  Memory reduced from {initial_memory:.2f} MB to {final_memory:.2f} MB ({reduction:.1f}% reduction)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_memory(*objects):\n",
    "    \"\"\"Explicitly clean objects from memory\"\"\"\n",
    "    for obj in objects:\n",
    "        del obj\n",
    "    gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. ENHANCED DATA LOADING WITH FILE CHECK\n",
    "# ==============================================================================\n",
    "\n",
    "def load_data_memory_efficient(filepath='simulations.csv', nrows=None, chunksize=50000):\n",
    "    \"\"\"Load data in chunks for memory efficiency with file existence check\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"MEMORY-EFFICIENT DATA LOADING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    file_path = Path(filepath)\n",
    "    if not file_path.exists():\n",
    "        print(f\"âš ï¸  File '{filepath}' not found.\")\n",
    "        print(f\"Creating synthetic dataset for demonstration...\")\n",
    "        return create_synthetic_dataset()\n",
    "    \n",
    "    try:\n",
    "        print(f\"Reading file structure from {filepath}...\")\n",
    "        sample = pd.read_csv(filepath, nrows=1000)\n",
    "        columns = sample.columns.tolist()\n",
    "        print(f\"Detected {len(columns)} columns\")\n",
    "        \n",
    "        # Enhanced dtype detection\n",
    "        dtype_dict = {}\n",
    "        for col in columns:\n",
    "            col_type = sample[col].dtype\n",
    "            if col_type == 'object':\n",
    "                # Check if it's actually numeric\n",
    "                try:\n",
    "                    pd.to_numeric(sample[col], errors='raise')\n",
    "                    dtype_dict[col] = 'float32'\n",
    "                except:\n",
    "                    # Check cardinality for categorical\n",
    "                    num_unique = sample[col].nunique()\n",
    "                    if num_unique / len(sample[col]) < 0.5:\n",
    "                        dtype_dict[col] = 'category'\n",
    "                    else:\n",
    "                        dtype_dict[col] = 'string'\n",
    "            elif col_type in ['int64', 'int32', 'int16']:\n",
    "                dtype_dict[col] = 'int32'\n",
    "            elif col_type in ['float64', 'float32']:\n",
    "                dtype_dict[col] = 'float32'\n",
    "            else:\n",
    "                dtype_dict[col] = col_type\n",
    "        \n",
    "        print(f\"Using optimized dtypes for memory efficiency\")\n",
    "        \n",
    "        # Load data\n",
    "        if nrows:\n",
    "            print(f\"Reading first {nrows:,} rows...\")\n",
    "            df = pd.read_csv(filepath, nrows=nrows, dtype=dtype_dict, low_memory=True)\n",
    "        else:\n",
    "            print(f\"Reading full file...\")\n",
    "            df = pd.read_csv(filepath, dtype=dtype_dict, low_memory=True)\n",
    "        \n",
    "        print(f\"\\nâœ… Loaded dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "        \n",
    "        # Optimize memory further\n",
    "        df = optimize_memory(df)\n",
    "        \n",
    "        # Print memory usage\n",
    "        memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        print(f\"Memory usage: {memory_mb:.2f} MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR loading data: {e}\")\n",
    "        print(\"Creating synthetic dataset for demonstration...\")\n",
    "        return create_synthetic_dataset()\n",
    "\n",
    "def create_synthetic_dataset(n_samples=100000):\n",
    "    \"\"\"Create synthetic dataset for demonstration\"\"\"\n",
    "    print(\"Creating synthetic Swiss dwellings dataset...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = {\n",
    "        'apartment_id': np.arange(n_samples),\n",
    "        'layout_area': np.random.lognormal(4.2, 0.35, n_samples).clip(15, 250).astype('float32'),\n",
    "        'layout_compactness': np.random.beta(3, 2, n_samples).clip(0.1, 0.95).astype('float32'),\n",
    "        'layout_window_perimeter': np.random.exponential(4, n_samples).clip(0, 25).astype('float32'),\n",
    "        'layout_number_of_windows': np.random.poisson(3, n_samples).clip(0, 10).astype('int8'),\n",
    "        'layout_room_count': np.random.choice([1, 2, 3, 4, 5, 6], n_samples, p=[0.05, 0.25, 0.4, 0.2, 0.08, 0.02]).astype('int8'),\n",
    "        'sun_201806211200_median': np.random.uniform(20, 180, n_samples).astype('float32'),\n",
    "        'view_sky_mean': np.random.beta(5, 2, n_samples).astype('float32'),\n",
    "        'view_greenery_mean': np.random.beta(3, 3, n_samples).astype('float32'),\n",
    "        'building_age': np.random.choice(range(1900, 2024, 10), n_samples).astype('int16'),\n",
    "        'altitude': np.random.exponential(500, n_samples).clip(200, 2500).astype('float32'),\n",
    "        'canton': np.random.choice(['ZH', 'BE', 'VD', 'GE', 'TI', 'AG', 'SG'], n_samples).astype('category'),\n",
    "        'layout_area_type': np.random.choice(['residential', 'commercial', 'mixed'], n_samples).astype('category')\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Created synthetic dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. ENHANCED APARTMENT AGGREGATION WITH CATEGORICAL HANDLING\n",
    "# ==============================================================================\n",
    "\n",
    "def aggregate_to_apartments_enhanced(df):\n",
    "    \"\"\"Enhanced aggregation with categorical feature handling\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ENHANCED APARTMENT AGGREGATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if 'apartment_id' not in df.columns:\n",
    "        print(\"No apartment_id found. Using rows as apartments.\")\n",
    "        return df\n",
    "    \n",
    "    print(\"Aggregating by apartment_id...\")\n",
    "    \n",
    "    # Group by apartment_id\n",
    "    grouped = df.groupby('apartment_id')\n",
    "    \n",
    "    # Aggregate numeric columns with mean\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'apartment_id' in numeric_cols:\n",
    "        numeric_cols.remove('apartment_id')\n",
    "    \n",
    "    numeric_agg = grouped[numeric_cols].mean()\n",
    "    \n",
    "    # Aggregate categorical columns with mode\n",
    "    categorical_cols = df.select_dtypes(include=['category', 'object', 'string']).columns.tolist()\n",
    "    if 'apartment_id' in categorical_cols:\n",
    "        categorical_cols.remove('apartment_id')\n",
    "    \n",
    "    categorical_agg = {}\n",
    "    for col in categorical_cols:\n",
    "        try:\n",
    "            categorical_agg[col] = grouped[col].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "        except:\n",
    "            categorical_agg[col] = grouped[col].agg(lambda x: x.iloc[0])\n",
    "    \n",
    "    # Combine results\n",
    "    apartment_df = numeric_agg.copy()\n",
    "    for col, series in categorical_agg.items():\n",
    "        apartment_df[col] = series\n",
    "    \n",
    "    apartment_df = apartment_df.reset_index()\n",
    "    \n",
    "    print(f\"âœ… Aggregated to {len(apartment_df):,} apartments\")\n",
    "    print(f\"  - Numeric features: {len(numeric_cols)}\")\n",
    "    print(f\"  - Categorical features: {len(categorical_cols)}\")\n",
    "    print(f\"Final shape: {apartment_df.shape}\")\n",
    "    \n",
    "    return apartment_df\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ENHANCED FEATURE ENGINEERING WITH CATEGORICAL ENCODING\n",
    "# ==============================================================================\n",
    "\n",
    "def engineer_features_enhanced(df):\n",
    "    \"\"\"Enhanced feature engineering with categorical encoding\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ENHANCED FEATURE ENGINEERING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    df_engineered = df.copy()\n",
    "    created_features = []\n",
    "    \n",
    "    # 1. Create area-to-window ratio\n",
    "    if 'layout_area' in df.columns and 'layout_window_perimeter' in df.columns:\n",
    "        df_engineered['area_window_ratio'] = (df_engineered['layout_area'] / \n",
    "                                            (df_engineered['layout_window_perimeter'] + 0.001)).astype('float32')\n",
    "        created_features.append('area_window_ratio')\n",
    "        print(\"  Created: area_window_ratio\")\n",
    "    \n",
    "    # 2. Create solar efficiency score\n",
    "    if 'sun_201806211200_median' in df.columns and 'layout_window_perimeter' in df.columns:\n",
    "        df_engineered['solar_efficiency'] = (df_engineered['sun_201806211200_median'] * \n",
    "                                           df_engineered['layout_window_perimeter']).astype('float32')\n",
    "        created_features.append('solar_efficiency')\n",
    "        print(\"  Created: solar_efficiency\")\n",
    "    \n",
    "    # 3. Create compactness efficiency score\n",
    "    if 'layout_compactness' in df.columns:\n",
    "        df_engineered['compactness_squared'] = (df_engineered['layout_compactness'] ** 2).astype('float32')\n",
    "        created_features.append('compactness_squared')\n",
    "        print(\"  Created: compactness_squared\")\n",
    "    \n",
    "    # 4. Create Swiss-specific features\n",
    "    if 'altitude' in df.columns:\n",
    "        # Energy impact of altitude (higher = colder = more heating)\n",
    "        df_engineered['altitude_energy_factor'] = (df_engineered['altitude'] / 1000).astype('float32')\n",
    "        created_features.append('altitude_energy_factor')\n",
    "        print(\"  Created: altitude_energy_factor\")\n",
    "    \n",
    "    # 5. Age-based features\n",
    "    if 'building_age' in df.columns:\n",
    "        current_year = 2024\n",
    "        df_engineered['building_age_years'] = (current_year - df_engineered['building_age']).astype('int16')\n",
    "        df_engineered['is_modern_building'] = (df_engineered['building_age_years'] < 30).astype('int8')\n",
    "        created_features.extend(['building_age_years', 'is_modern_building'])\n",
    "        print(\"  Created: building_age_years, is_modern_building\")\n",
    "    \n",
    "    print(f\"\\nâœ… Created {len(created_features)} engineered features\")\n",
    "    print(f\"Total features now: {df_engineered.shape[1]}\")\n",
    "    \n",
    "    return df_engineered, created_features\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. IMPROVED REALISTIC ENERGY TARGET CREATION\n",
    "# ==============================================================================\n",
    "\n",
    "def create_improved_realistic_energy_target(df, key_features_list=None):\n",
    "    \"\"\"Create improved realistic energy consumption target with proper distribution\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CREATING IMPROVED REALISTIC ENERGY TARGET\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = len(df)\n",
    "    \n",
    "    # Create more realistic distribution with better spread\n",
    "    # Use a mix of normal and log-normal distributions\n",
    "    \n",
    "    # Base distribution - mixture of normal distributions for different building types\n",
    "    # Modern buildings (30%): more efficient\n",
    "    modern_mask = np.random.random(n_samples) < 0.3\n",
    "    # Old buildings (40%): less efficient  \n",
    "    old_mask = np.random.random(n_samples) < 0.4\n",
    "    # Standard buildings (30%): average efficiency\n",
    "    standard_mask = ~modern_mask & ~old_mask\n",
    "    \n",
    "    # Different base energy distributions for each type\n",
    "    base_energy = np.zeros(n_samples)\n",
    "    \n",
    "    # Modern buildings: 60-120 kWh/mÂ²/year\n",
    "    if modern_mask.any():\n",
    "        modern_energy = np.random.normal(90, 15, modern_mask.sum())\n",
    "        modern_energy = np.clip(modern_energy, 60, 120)\n",
    "        base_energy[modern_mask] = modern_energy\n",
    "    \n",
    "    # Old buildings: 120-250 kWh/mÂ²/year\n",
    "    if old_mask.any():\n",
    "        old_energy = np.random.lognormal(4.8, 0.3, old_mask.sum())\n",
    "        old_energy = np.clip(old_energy, 120, 250)\n",
    "        base_energy[old_mask] = old_energy\n",
    "    \n",
    "    # Standard buildings: 80-180 kWh/mÂ²/year\n",
    "    if standard_mask.any():\n",
    "        standard_energy = np.random.normal(130, 25, standard_mask.sum())\n",
    "        standard_energy = np.clip(standard_energy, 80, 180)\n",
    "        base_energy[standard_mask] = standard_energy\n",
    "    \n",
    "    # Add feature-based adjustments with more impact\n",
    "    if key_features_list:\n",
    "        for feature in key_features_list:\n",
    "            if feature in df.columns:\n",
    "                values = df[feature].fillna(df[feature].median()).values\n",
    "                \n",
    "                if np.std(values) > 0:\n",
    "                    # Standardize and scale to have meaningful impact\n",
    "                    standardized = (values - np.mean(values)) / np.std(values)\n",
    "                    # Limit extreme values\n",
    "                    standardized = np.clip(standardized, -2, 2)\n",
    "                else:\n",
    "                    standardized = np.zeros_like(values)\n",
    "                \n",
    "                # Feature-specific impact coefficients - increased for more variation\n",
    "                impact_coefficient = 0\n",
    "                \n",
    "                if 'area' in feature.lower():\n",
    "                    impact_coefficient = 50  # Larger area = more energy\n",
    "                elif 'compact' in feature.lower():\n",
    "                    impact_coefficient = -30  # More compact = better insulation\n",
    "                elif 'window' in feature.lower():\n",
    "                    impact_coefficient = 40   # More windows = more heat loss\n",
    "                elif 'perimeter' in feature.lower():\n",
    "                    impact_coefficient = 35   # Larger perimeter = more surface area\n",
    "                elif 'sun' in feature.lower():\n",
    "                    impact_coefficient = -25  # More sun = passive solar gain\n",
    "                elif 'view' in feature.lower():\n",
    "                    impact_coefficient = -15  # Better views might correlate with better design\n",
    "                elif 'altitude' in feature.lower():\n",
    "                    impact_coefficient = 30   # Higher altitude = colder = more heating\n",
    "                elif 'age' in feature.lower() or 'year' in feature.lower():\n",
    "                    impact_coefficient = 25   # Older buildings = less efficient\n",
    "                \n",
    "                if impact_coefficient != 0:\n",
    "                    base_energy += standardized * impact_coefficient\n",
    "    \n",
    "    # Ensure realistic bounds\n",
    "    base_energy = np.clip(base_energy, 40, 350).astype('float32')\n",
    "    \n",
    "    # Convert to annual energy (kWh/year)\n",
    "    if 'layout_area' in df.columns:\n",
    "        area = df['layout_area'].fillna(df['layout_area'].median()).values\n",
    "        # Non-linear area efficiency\n",
    "        area_factor = 0.9 + 0.3 * np.log1p(area / 100)  # Efficiency curve\n",
    "        annual_energy = base_energy * area * area_factor\n",
    "    else:\n",
    "        annual_energy = base_energy * 80\n",
    "    \n",
    "    # Add realistic variation\n",
    "    # Building quality variation (25%)\n",
    "    quality_factor = np.random.normal(1.0, 0.125, n_samples)\n",
    "    # Occupancy variation (15%)\n",
    "    occupancy_factor = np.random.normal(1.0, 0.075, n_samples)\n",
    "    # Regional climate variation\n",
    "    climate_factor = np.random.choice([0.85, 1.0, 1.15], n_samples, p=[0.2, 0.6, 0.2])\n",
    "    \n",
    "    annual_energy = annual_energy * quality_factor * occupancy_factor * climate_factor\n",
    "    \n",
    "    # Apply final realistic bounds\n",
    "    annual_energy = np.clip(annual_energy, 3000, 40000).astype('int32')\n",
    "    \n",
    "    # Add some random noise for realism\n",
    "    noise = np.random.normal(0, annual_energy.std() * 0.05, n_samples)\n",
    "    annual_energy = (annual_energy + noise).astype('int32')\n",
    "    annual_energy = np.clip(annual_energy, 3000, 40000)\n",
    "    \n",
    "    # Create target series\n",
    "    target = pd.Series(annual_energy, index=df.index, name='energy_consumption_kwh')\n",
    "    \n",
    "    print(f\"âœ… IMPROVED REALISTIC Target Statistics:\")\n",
    "    print(f\"  Mean: {target.mean():,.0f} kWh/year\")\n",
    "    print(f\"  Std:  {target.std():,.0f} kWh/year\")\n",
    "    print(f\"  Min:  {target.min():,.0f} kWh/year\")\n",
    "    print(f\"  Max:  {target.max():,.0f} kWh/year\")\n",
    "    print(f\"  25th percentile: {np.percentile(target, 25):,.0f} kWh/year\")\n",
    "    print(f\"  Median: {np.percentile(target, 50):,.0f} kWh/year\")\n",
    "    print(f\"  75th percentile: {np.percentile(target, 75):,.0f} kWh/year\")\n",
    "    print(f\"  Coefficient of Variation: {(target.std()/target.mean()*100):.1f}%\")\n",
    "    \n",
    "    # Check if distribution is realistic (not all at minimum)\n",
    "    unique_values = target.nunique()\n",
    "    if unique_values < 100:\n",
    "        print(f\"âš ï¸  Warning: Only {unique_values} unique values in target\")\n",
    "    \n",
    "    # Create distribution plot\n",
    "    create_target_distribution_plot(target)\n",
    "    \n",
    "    return target\n",
    "\n",
    "def create_target_distribution_plot(target):\n",
    "    \"\"\"Create target distribution visualization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Histogram\n",
    "    axes[0, 0].hist(target, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].axvline(target.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {target.mean():,.0f}')\n",
    "    axes[0, 0].axvline(np.median(target), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(target):,.0f}')\n",
    "    axes[0, 0].set_xlabel('Annual Energy Consumption (kWh/year)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Energy Consumption Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Box plot\n",
    "    axes[0, 1].boxplot(target, vert=False)\n",
    "    axes[0, 1].set_xlabel('Annual Energy Consumption (kWh/year)')\n",
    "    axes[0, 1].set_title('Energy Consumption Statistics')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. ECDF plot\n",
    "    sorted_data = np.sort(target)\n",
    "    ecdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "    axes[1, 0].plot(sorted_data, ecdf, 'b-', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Annual Energy Consumption (kWh/year)')\n",
    "    axes[1, 0].set_ylabel('Cumulative Probability')\n",
    "    axes[1, 0].set_title('Empirical Cumulative Distribution Function')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Statistics summary\n",
    "    stats_text = f\"\"\"\n",
    "    Statistical Summary:\n",
    "    \n",
    "    Basic Statistics:\n",
    "    â€¢ Mean: {target.mean():,.0f} kWh/year\n",
    "    â€¢ Std Dev: {target.std():,.0f} kWh/year\n",
    "    â€¢ CV: {(target.std()/target.mean()*100):.1f}%\n",
    "    \n",
    "    Percentiles:\n",
    "    â€¢ 10th: {np.percentile(target, 10):,.0f} kWh/year\n",
    "    â€¢ 25th: {np.percentile(target, 25):,.0f} kWh/year\n",
    "    â€¢ 50th: {np.percentile(target, 50):,.0f} kWh/year\n",
    "    â€¢ 75th: {np.percentile(target, 75):,.0f} kWh/year\n",
    "    â€¢ 90th: {np.percentile(target, 90):,.0f} kWh/year\n",
    "    \n",
    "    Distribution:\n",
    "    â€¢ Unique values: {target.nunique()}\n",
    "    â€¢ Range: {target.max() - target.min():,.0f} kWh/year\n",
    "    â€¢ IQR: {np.percentile(target, 75) - np.percentile(target, 25):,.0f} kWh/year\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.05, 0.95, stats_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=10, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Swiss Dwellings - Improved Realistic Energy Consumption Analysis', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/plots/energy_distribution_improved.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nâœ… Improved distribution plot saved to output/plots/energy_distribution_improved.png\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. ENHANCED FEATURE SELECTION WITH VARIANCE THRESHOLD\n",
    "# ==============================================================================\n",
    "\n",
    "def select_features_enhanced(df, target, max_features=20, variance_threshold=0.01):\n",
    "    \"\"\"Enhanced feature selection with variance threshold\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ENHANCED FEATURE SELECTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Remove low-variance features first\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    df_numeric = df[numeric_cols].copy()\n",
    "    \n",
    "    # Apply variance threshold\n",
    "    variances = df_numeric.var()\n",
    "    high_variance_cols = variances[variances > variance_threshold].index.tolist()\n",
    "    \n",
    "    print(f\"Variance filtering: {len(numeric_cols)} â†’ {len(high_variance_cols)} features\")\n",
    "    \n",
    "    # Calculate correlations for high-variance features\n",
    "    correlations = {}\n",
    "    for col in high_variance_cols:\n",
    "        try:\n",
    "            vals = df[col].fillna(df[col].median()).values\n",
    "            if np.std(vals) > 0:\n",
    "                corr = np.corrcoef(vals, target.values)[0, 1]\n",
    "                if not np.isnan(corr):\n",
    "                    correlations[col] = abs(corr)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select top features\n",
    "    selected_features = [col for col, corr in sorted_correlations[:max_features]]\n",
    "    \n",
    "    print(f\"âœ… Selected {len(selected_features)} features:\")\n",
    "    for i, (col, corr) in enumerate(sorted_correlations[:10]):\n",
    "        direction = \"+\" if corr > 0 else \"-\"\n",
    "        print(f\"  {i+1:2d}. {col:40s}: {corr:6.3f} ({direction})\")\n",
    "    \n",
    "    # Include important categorical features (but exclude apartment_id for modeling)\n",
    "    categorical_cols = df.select_dtypes(include=['category']).columns.tolist()\n",
    "    \n",
    "    # Filter out apartment_id from categorical features for modeling\n",
    "    categorical_features_for_modeling = [col for col in categorical_cols if col != 'apartment_id']\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"\\nCategorical features available: {', '.join(categorical_cols[:5])}\")\n",
    "        if categorical_features_for_modeling:\n",
    "            print(f\"  Using for modeling: {categorical_features_for_modeling}\")\n",
    "        else:\n",
    "            print(f\"  No categorical features suitable for modeling (excluding apartment_id)\")\n",
    "    \n",
    "    # Create correlation visualization\n",
    "    if len(sorted_correlations) > 5:\n",
    "        top_features = [col for col, corr in sorted_correlations[:10]]\n",
    "        top_corrs = [corr for col, corr in sorted_correlations[:10]]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.barh(range(len(top_features)), top_corrs, color='lightcoral')\n",
    "        plt.yticks(range(len(top_features)), top_features)\n",
    "        plt.xlabel('Absolute Correlation with Energy Consumption')\n",
    "        plt.title('Top 10 Features by Correlation (Variance Filtered)')\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        for i, (bar, corr) in enumerate(zip(bars, top_corrs)):\n",
    "            plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{corr:.3f}', ha='left', va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('output/plots/feature_correlations_enhanced.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\nâœ… Correlation plot saved to output/plots/feature_correlations_enhanced.png\")\n",
    "    \n",
    "    return selected_features, categorical_features_for_modeling\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. MODEL COMPARISON PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "def build_model_comparison(numeric_features, categorical_features=None):\n",
    "    \"\"\"Build multiple models for comparison\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BUILDING MODEL COMPARISON PIPELINE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Define transformers\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    transformers = [('numeric', numeric_transformer, numeric_features)]\n",
    "    \n",
    "    if categorical_features:\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "        transformers.append(('categorical', categorical_transformer, categorical_features))\n",
    "        print(f\"  Including {len(categorical_features)} categorical features\")\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "    \n",
    "    # Define models with different characteristics\n",
    "    models = {\n",
    "        'GradientBoosting': GradientBoostingRegressor(\n",
    "            n_estimators=150, \n",
    "            learning_rate=0.1, \n",
    "            max_depth=4,\n",
    "            min_samples_split=10, \n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            subsample=0.8\n",
    "        ),\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            n_estimators=100, \n",
    "            max_depth=10,\n",
    "            min_samples_split=10, \n",
    "            min_samples_leaf=5,\n",
    "            random_state=42, \n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Ridge': Ridge(\n",
    "            alpha=1.0, \n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Create pipelines\n",
    "    pipelines = {}\n",
    "    for name, model in models.items():\n",
    "        pipelines[name] = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "    \n",
    "    # Feature summary\n",
    "    feature_summary = f\"{len(numeric_features)} numeric\"\n",
    "    if categorical_features:\n",
    "        feature_summary += f\" + {len(categorical_features)} categorical\"\n",
    "    \n",
    "    print(f\"âœ… Model comparison configured with {feature_summary} features\")\n",
    "    print(f\"Models: {', '.join(pipelines.keys())}\")\n",
    "    \n",
    "    return pipelines\n",
    "\n",
    "def compare_models(pipelines, X_train, X_test, y_train, y_test, feature_names, y_test_actual=None):\n",
    "    \"\"\"Compare multiple models and select the best one\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"MODEL COMPARISON EVALUATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, pipeline in pipelines.items():\n",
    "        print(f\"\\n{'â”€'*40}\")\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        print(f\"{'â”€'*40}\")\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        try:\n",
    "            cv_scores = cross_val_score(pipeline, X_train, y_train, \n",
    "                                       cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                                       scoring='r2',\n",
    "                                       n_jobs=-1)\n",
    "            \n",
    "            print(f\"Cross-validation scores: {cv_scores}\")\n",
    "            print(f\"Mean CV RÂ²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Cross-validation failed: {e}\")\n",
    "            cv_scores = np.array([0, 0, 0, 0, 0])\n",
    "        \n",
    "        # Train final model\n",
    "        try:\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred_train = pipeline.predict(X_train)\n",
    "            y_pred_test = pipeline.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'train_r2': float(r2_score(y_train, y_pred_train)),\n",
    "                'test_r2': float(r2_score(y_test, y_pred_test)),\n",
    "                'train_mae': float(mean_absolute_error(y_train, y_pred_train)),\n",
    "                'test_mae': float(mean_absolute_error(y_test, y_pred_test)),\n",
    "                'train_rmse': float(np.sqrt(mean_squared_error(y_train, y_pred_train))),\n",
    "                'test_rmse': float(np.sqrt(mean_squared_error(y_test, y_pred_test))),\n",
    "                'train_mape': float(np.mean(np.abs((y_train - y_pred_train) / y_train)) * 100),\n",
    "                'test_mape': float(np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100),\n",
    "                'cv_mean_r2': float(cv_scores.mean()),\n",
    "                'cv_std_r2': float(cv_scores.std())\n",
    "            }\n",
    "            \n",
    "            print(f\"Test RÂ²: {metrics['test_r2']:.4f}\")\n",
    "            print(f\"Test MAE: {metrics['test_mae']:,.0f} kWh/year\")\n",
    "            print(f\"Test MAPE: {metrics['test_mape']:.1f}%\")\n",
    "            \n",
    "            results[name] = {\n",
    "                'pipeline': pipeline,\n",
    "                'metrics': metrics,\n",
    "                'predictions': {\n",
    "                    'train': y_pred_train,\n",
    "                    'test': y_pred_test\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Model training failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âŒ No models were successfully trained!\")\n",
    "        return {}, None, None\n",
    "    \n",
    "    # Find best model based on test RÂ²\n",
    "    best_model_name = max(results.keys(), key=lambda x: results[x]['metrics']['test_r2'])\n",
    "    best_model = results[best_model_name]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ† BEST MODEL: {best_model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test RÂ²: {best_model['metrics']['test_r2']:.4f}\")\n",
    "    print(f\"Test MAE: {best_model['metrics']['test_mae']:,.0f} kWh/year\")\n",
    "    print(f\"CV Mean RÂ²: {best_model['metrics']['cv_mean_r2']:.4f}\")\n",
    "    \n",
    "    # Create model comparison plot\n",
    "    try:\n",
    "        create_model_comparison_plot(results, y_test_actual if y_test_actual is not None else y_test)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Model comparison plot creation failed: {e}\")\n",
    "    \n",
    "    # Perform detailed feature importance analysis for tree-based models\n",
    "    for name in ['GradientBoosting', 'RandomForest']:\n",
    "        if name in results:\n",
    "            print(f\"\\n{'â”€'*40}\")\n",
    "            print(f\"Feature Importance Analysis for {name}\")\n",
    "            print(f\"{'â”€'*40}\")\n",
    "            try:\n",
    "                importance_df = analyze_feature_importance(results[name]['pipeline'], feature_names, X_train)\n",
    "                if importance_df is not None:\n",
    "                    # Save importance analysis\n",
    "                    importance_df.to_csv(f'output/model_comparison/feature_importance_{name.lower()}.csv', index=False)\n",
    "                    print(f\"âœ… Feature importance saved for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Feature importance analysis failed for {name}: {e}\")\n",
    "    \n",
    "    return results, best_model_name, best_model\n",
    "\n",
    "def create_model_comparison_plot(results, y_test):\n",
    "    \"\"\"Create visualization comparing all models\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    models = list(results.keys())\n",
    "    \n",
    "    # 1. Test RÂ² comparison\n",
    "    test_r2_values = [results[m]['metrics']['test_r2'] for m in models]\n",
    "    cv_r2_values = [results[m]['metrics']['cv_mean_r2'] for m in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 0].bar(x - width/2, test_r2_values, width, label='Test RÂ²', color='skyblue')\n",
    "    axes[0, 0].bar(x + width/2, cv_r2_values, width, label='CV Mean RÂ²', color='lightcoral')\n",
    "    axes[0, 0].set_xlabel('Model')\n",
    "    axes[0, 0].set_ylabel('RÂ² Score')\n",
    "    axes[0, 0].set_title('Model Performance Comparison (RÂ²)')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(models, rotation=45)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (test_r2, cv_r2) in enumerate(zip(test_r2_values, cv_r2_values)):\n",
    "        axes[0, 0].text(i - width/2, test_r2 + 0.01, f'{test_r2:.3f}', \n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "        axes[0, 0].text(i + width/2, cv_r2 + 0.01, f'{cv_r2:.3f}', \n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 2. Error metrics comparison\n",
    "    mae_values = [results[m]['metrics']['test_mae'] for m in models]\n",
    "    mape_values = [results[m]['metrics']['test_mape'] for m in models]\n",
    "    \n",
    "    # Normalize for better visualization\n",
    "    if max(mae_values) > 0:\n",
    "        mae_norm = [v/max(mae_values) for v in mae_values]\n",
    "    else:\n",
    "        mae_norm = [0 for _ in mae_values]\n",
    "    \n",
    "    if max(mape_values) > 0:\n",
    "        mape_norm = [v/max(mape_values) for v in mape_values]\n",
    "    else:\n",
    "        mape_norm = [0 for _ in mape_values]\n",
    "    \n",
    "    axes[0, 1].bar(x - width/2, mae_norm, width, label='MAE (normalized)', color='lightgreen')\n",
    "    axes[0, 1].bar(x + width/2, mape_norm, width, label='MAPE (normalized)', color='orange')\n",
    "    axes[0, 1].set_xlabel('Model')\n",
    "    axes[0, 1].set_ylabel('Normalized Error')\n",
    "    axes[0, 1].set_title('Error Metrics Comparison (Normalized)')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(models, rotation=45)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add actual values\n",
    "    for i, (mae, mape) in enumerate(zip(mae_values, mape_values)):\n",
    "        axes[0, 1].text(i - width/2, mae_norm[i] + 0.02, f'{mae:.0f}', \n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "        axes[0, 1].text(i + width/2, mape_norm[i] + 0.02, f'{mape:.1f}%', \n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 3. Actual vs Predicted for best model\n",
    "    best_model_name = max(results.keys(), key=lambda x: results[x]['metrics']['test_r2'])\n",
    "    best_result = results[best_model_name]\n",
    "    y_pred_test = best_result['predictions']['test']\n",
    "    \n",
    "    # Plot first 100 samples for clarity\n",
    "    n_samples = min(100, len(y_pred_test))\n",
    "    sample_indices = range(n_samples)\n",
    "    \n",
    "    axes[1, 0].scatter(sample_indices, y_pred_test[:n_samples], \n",
    "                      alpha=0.6, s=30, color='blue', label='Predictions')\n",
    "    axes[1, 0].scatter(sample_indices, y_test.values[:n_samples],\n",
    "                      alpha=0.6, s=30, color='red', label='Actual', marker='x')\n",
    "    axes[1, 0].set_xlabel('Sample Index')\n",
    "    axes[1, 0].set_ylabel('Energy (kWh/year)')\n",
    "    axes[1, 0].set_title(f'{best_model_name} - Test Predictions vs Actual (First {n_samples})')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Model characteristics\n",
    "    model_info = {\n",
    "        'GradientBoosting': 'Tree ensemble, non-linear, handles interactions',\n",
    "        'RandomForest': 'Tree ensemble, robust, handles outliers',\n",
    "        'Ridge': 'Linear, interpretable, fast'\n",
    "    }\n",
    "    \n",
    "    info_text = \"Model Characteristics:\\n\\n\"\n",
    "    for model in models:\n",
    "        info_text += f\"â€¢ {model}:\\n  {model_info.get(model, 'No description')}\\n\\n\"\n",
    "    \n",
    "    axes[1, 1].text(0.05, 0.95, info_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=9, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Swiss Dwellings - Model Comparison Analysis', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/model_comparison/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nâœ… Model comparison plot saved to output/model_comparison/model_comparison.png\")\n",
    "\n",
    "def analyze_feature_importance(pipeline, feature_names, X_train):\n",
    "    \"\"\"Analyze feature importance in detail for tree-based models\"\"\"\n",
    "    \n",
    "    # Check if model has feature_importances_ attribute\n",
    "    if not hasattr(pipeline.named_steps['model'], 'feature_importances_'):\n",
    "        print(\"  Model doesn't support feature importance analysis\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        importances = pipeline.named_steps['model'].feature_importances_\n",
    "        \n",
    "        # Handle transformed feature names\n",
    "        preprocessor = pipeline.named_steps['preprocessor']\n",
    "        \n",
    "        # Get all feature names after preprocessing\n",
    "        all_feature_names = []\n",
    "        \n",
    "        # Get numeric feature names\n",
    "        if 'numeric' in preprocessor.named_transformers_:\n",
    "            all_feature_names.extend(feature_names)\n",
    "        \n",
    "        # Get categorical feature names if they exist\n",
    "        if 'categorical' in preprocessor.named_transformers_:\n",
    "            categorical_transformer = preprocessor.named_transformers_['categorical']\n",
    "            \n",
    "            # Check if encoder exists in the pipeline\n",
    "            if hasattr(categorical_transformer, 'named_steps') and 'encoder' in categorical_transformer.named_steps:\n",
    "                encoder = categorical_transformer.named_steps['encoder']\n",
    "                \n",
    "                # Get categorical feature indices from ColumnTransformer\n",
    "                for name, transformer, cols in preprocessor.transformers:\n",
    "                    if name == 'categorical':\n",
    "                        categorical_features = cols\n",
    "                        break\n",
    "                else:\n",
    "                    categorical_features = []\n",
    "                \n",
    "                # Get one-hot encoded feature names\n",
    "                if hasattr(encoder, 'get_feature_names_out'):\n",
    "                    if categorical_features:\n",
    "                        encoded_cat_names = encoder.get_feature_names_out(categorical_features)\n",
    "                        all_feature_names.extend(encoded_cat_names)\n",
    "        \n",
    "        # Ensure we have the right number of feature names\n",
    "        if len(all_feature_names) > len(importances):\n",
    "            all_feature_names = all_feature_names[:len(importances)]\n",
    "        elif len(all_feature_names) < len(importances):\n",
    "            # Create generic names for missing ones\n",
    "            for i in range(len(all_feature_names), len(importances)):\n",
    "                all_feature_names.append(f'feature_{i}')\n",
    "        \n",
    "        # Create importance DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': all_feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Plot top features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_n = min(20, len(importance_df))\n",
    "        top_features = importance_df.head(top_n)\n",
    "        \n",
    "        plt.barh(range(top_n), top_features['importance'][::-1], color='lightblue', edgecolor='darkblue')\n",
    "        plt.yticks(range(top_n), top_features['feature'][::-1], fontsize=10)\n",
    "        plt.xlabel('Feature Importance', fontsize=12)\n",
    "        plt.title(f'Top {top_n} Most Important Features', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add importance values\n",
    "        for i, (_, row) in enumerate(top_features[::-1].iterrows()):\n",
    "            plt.text(row['importance'] + 0.001, i, f'{row[\"importance\"]:.4f}', \n",
    "                    va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Get model name for filename\n",
    "        model_name = pipeline.named_steps['model'].__class__.__name__\n",
    "        plt.savefig(f'output/model_comparison/feature_importance_{model_name}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"âœ… Top 10 features by importance:\")\n",
    "        for i, row in importance_df.head(10).iterrows():\n",
    "            print(f\"  {i+1:2d}. {row['feature']:40s}: {row['importance']:.4f}\")\n",
    "        \n",
    "        return importance_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error in feature importance analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. FINAL ENHANCED MAIN EXECUTION PIPELINE (COMPLETELY FIXED)\n",
    "# ==============================================================================\n",
    "\n",
    "def run_final_enhanced_analysis():\n",
    "    \"\"\"Run final enhanced memory-optimized analysis pipeline - COMPLETELY FIXED\"\"\"\n",
    "    print(\"=\" * 100)\n",
    "    print(\"SWISS DWELLINGS - FINAL ENHANCED ANALYSIS PIPELINE\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 1: LOADING DATA\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        df = load_data_memory_efficient('simulations.csv', nrows=100000)\n",
    "        \n",
    "        # Step 2: Aggregate to apartments\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 2: APARTMENT AGGREGATION\")\n",
    "        print(\"=\"*60)\n",
    "        apartment_df = aggregate_to_apartments_enhanced(df)\n",
    "        \n",
    "        # Clean up memory\n",
    "        clean_memory(df)\n",
    "        \n",
    "        # Step 3: Feature engineering\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 3: FEATURE ENGINEERING\")\n",
    "        print(\"=\"*60)\n",
    "        engineered_df, engineered_features = engineer_features_enhanced(apartment_df)\n",
    "        \n",
    "        # Clean up memory\n",
    "        clean_memory(apartment_df)\n",
    "        \n",
    "        # Step 4: Identify key features\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 4: IDENTIFYING KEY FEATURES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        key_features = []\n",
    "        feature_patterns = ['area', 'compact', 'window', 'perimeter', 'room', 'sun', 'view', 'altitude', 'age']\n",
    "        \n",
    "        for pattern in feature_patterns:\n",
    "            matching = [col for col in engineered_df.columns if pattern in col.lower()]\n",
    "            if matching:\n",
    "                key_features.extend(matching[:2])\n",
    "        \n",
    "        key_features = list(set(key_features))\n",
    "        \n",
    "        print(f\"Identified {len(key_features)} key features:\")\n",
    "        for i, feat in enumerate(key_features[:10], 1):\n",
    "            print(f\"  {i:2d}. {feat}\")\n",
    "        \n",
    "        # Step 5: Create improved realistic target\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 5: CREATING IMPROVED REALISTIC ENERGY TARGET\")\n",
    "        print(\"=\"*60)\n",
    "        target = create_improved_realistic_energy_target(engineered_df, key_features)\n",
    "        \n",
    "        # Step 6: Feature selection\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 6: FEATURE SELECTION\")\n",
    "        print(\"=\"*60)\n",
    "        numeric_features, categorical_features = select_features_enhanced(\n",
    "            engineered_df, target, max_features=15\n",
    "        )\n",
    "        \n",
    "        # Step 7: Prepare data\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 7: PREPARING DATA FOR MODELING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Use selected features\n",
    "        modeling_features = numeric_features + categorical_features\n",
    "        X = engineered_df[modeling_features].copy()\n",
    "        y = target  # Store the full target for saving\n",
    "        \n",
    "        # Handle missing values with appropriate strategies\n",
    "        X[numeric_features] = X[numeric_features].fillna(X[numeric_features].median())\n",
    "        if categorical_features:\n",
    "            for cat_col in categorical_features:\n",
    "                mode_values = X[cat_col].mode()\n",
    "                if not mode_values.empty:\n",
    "                    X[cat_col] = X[cat_col].fillna(mode_values.iloc[0])\n",
    "                else:\n",
    "                    X[cat_col] = X[cat_col].fillna('Unknown')\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Data prepared:\")\n",
    "        print(f\"  Training set: {len(X_train):,} samples\")\n",
    "        print(f\"  Test set:     {len(X_test):,} samples\")\n",
    "        print(f\"  Features:     {len(numeric_features)} numeric\" + \n",
    "              (f\", {len(categorical_features)} categorical\" if categorical_features else \"\"))\n",
    "        \n",
    "        # Step 8: Build model comparison pipeline\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 8: BUILDING MODEL COMPARISON PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        pipelines = build_model_comparison(numeric_features, categorical_features)\n",
    "        \n",
    "        # Step 9: Compare models and select best\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 9: MODEL COMPARISON AND SELECTION\")\n",
    "        print(\"=\"*60)\n",
    "        all_results, best_model_name, best_model = compare_models(\n",
    "            pipelines, X_train, X_test, y_train, y_test, modeling_features, y_test\n",
    "        )\n",
    "        \n",
    "        if not all_results:\n",
    "            print(\"âŒ No models were successfully trained. Creating basic output...\")\n",
    "            # Create at least some basic output\n",
    "            create_basic_output(engineered_df, target)\n",
    "            return None\n",
    "        \n",
    "        # Step 10: Save results - PASS THE FULL TARGET 'y'\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 10: SAVING FINAL RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        save_final_results(all_results, best_model_name, best_model, engineered_df, \n",
    "                          X_train, X_test, y_train, y_test, y,  # PASS y HERE\n",
    "                          numeric_features, categorical_features)\n",
    "        \n",
    "        # Calculate execution time\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Final summary\n",
    "        print_final_summary_enhanced(engineered_df, y, numeric_features, \n",
    "                                    categorical_features, all_results, \n",
    "                                    best_model_name, execution_time)\n",
    "        \n",
    "        return {\n",
    "            'all_results': all_results,\n",
    "            'best_model': best_model,\n",
    "            'best_model_name': best_model_name,\n",
    "            'features': {\n",
    "                'numeric': numeric_features,\n",
    "                'categorical': categorical_features\n",
    "            },\n",
    "            'data': engineered_df,\n",
    "            'target': y\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ERROR in final enhanced pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        save_error_report(e, traceback)\n",
    "        # Create basic output even if pipeline fails\n",
    "        try:\n",
    "            if 'engineered_df' in locals() and 'target' in locals():\n",
    "                create_basic_output(engineered_df, target)\n",
    "            elif 'engineered_df' in locals():\n",
    "                create_basic_output(engineered_df, None)\n",
    "            else:\n",
    "                create_basic_output(None, None)\n",
    "        except:\n",
    "            print(\"âš ï¸  Could not create basic output\")\n",
    "        return None\n",
    "\n",
    "def create_basic_output(engineered_df=None, target=None):\n",
    "    \"\"\"Create basic outputs if main pipeline fails\"\"\"\n",
    "    print(\"\\nCreating basic demonstration output...\")\n",
    "    \n",
    "    try:\n",
    "        # Create a simple analysis if we have data\n",
    "        if engineered_df is not None and target is not None:\n",
    "            # Basic statistics\n",
    "            stats = {\n",
    "                'n_apartments': len(engineered_df),\n",
    "                'target_mean': float(target.mean()),\n",
    "                'target_std': float(target.std()),\n",
    "                'target_min': float(target.min()),\n",
    "                'target_max': float(target.max())\n",
    "            }\n",
    "            \n",
    "            # Save basic report\n",
    "            basic_report = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'status': 'basic_output_created',\n",
    "                'statistics': stats,\n",
    "                'message': 'Main pipeline failed, basic output created'\n",
    "            }\n",
    "            \n",
    "            with open('output/reports/basic_report.json', 'w') as f:\n",
    "                json.dump(basic_report, f, indent=2)\n",
    "            \n",
    "            print(\"âœ… Basic report saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not create basic output: {e}\")\n",
    "\n",
    "def save_final_results(all_results, best_model_name, best_model, engineered_df, \n",
    "                      X_train, X_test, y_train, y_test, y_full,  # ADDED y_full parameter\n",
    "                      numeric_features, categorical_features):\n",
    "    \"\"\"Save final enhanced results - FIXED with y_full parameter\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # 1. Save the best model\n",
    "    model_data = {\n",
    "        'model': best_model['pipeline'],\n",
    "        'model_name': best_model_name,\n",
    "        'features': {\n",
    "            'numeric': numeric_features,\n",
    "            'categorical': categorical_features\n",
    "        },\n",
    "        'metrics': best_model['metrics'],\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'version': f'2.2_{timestamp}',\n",
    "        'data_summary': {\n",
    "            'n_samples': len(engineered_df),\n",
    "            'n_features': len(numeric_features) + len(categorical_features),\n",
    "            'target_mean': float(y_train.mean()),\n",
    "            'target_std': float(y_train.std())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if hasattr(best_model['pipeline'].named_steps['model'], 'feature_importances_'):\n",
    "        model_data['feature_importances'] = best_model['pipeline'].named_steps['model'].feature_importances_.tolist()\n",
    "    \n",
    "    best_model_filename = f'output/models/best_model_{best_model_name.lower()}_{timestamp}.pkl'\n",
    "    joblib.dump(model_data, best_model_filename)\n",
    "    print(f\"âœ… Best model saved: {best_model_filename}\")\n",
    "    \n",
    "    # 2. Save all models comparison results\n",
    "    comparison_results = {}\n",
    "    for model_name, result in all_results.items():\n",
    "        comparison_results[model_name] = {\n",
    "            'metrics': result['metrics'],\n",
    "            'model_type': result['pipeline'].named_steps['model'].__class__.__name__\n",
    "        }\n",
    "    \n",
    "    comparison_filename = f'output/model_comparison/model_comparison_results_{timestamp}.json'\n",
    "    with open(comparison_filename, 'w') as f:\n",
    "        json.dump(convert_to_json_serializable(comparison_results), f, indent=2)\n",
    "    print(f\"âœ… Model comparison results saved: {comparison_filename}\")\n",
    "    \n",
    "    # 3. Save processed data - FIXED: using y_full instead of undefined y\n",
    "    processed_data = engineered_df[numeric_features + categorical_features].copy()\n",
    "    processed_data['energy_consumption_kwh'] = y_full\n",
    "    processed_filename = f'output/data/swiss_dwellings_processed_final_{timestamp}.csv'\n",
    "    processed_data.to_csv(processed_filename, index=False)\n",
    "    print(f\"âœ… Processed data saved: {processed_filename}\")\n",
    "    \n",
    "    # 4. Save comprehensive report\n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'project': 'Swiss Dwellings Energy Analysis - Final Enhanced (Completely Fixed)',\n",
    "        'version': f'2.2_{timestamp}',\n",
    "        'best_model': best_model_name,\n",
    "        'best_model_performance': best_model['metrics'],\n",
    "        'model_comparison': comparison_results,\n",
    "        'execution_summary': {\n",
    "            'train_samples': int(len(X_train)),\n",
    "            'test_samples': int(len(X_test)),\n",
    "            'features_selected': {\n",
    "                'numeric': int(len(numeric_features)),\n",
    "                'categorical': int(len(categorical_features))\n",
    "            },\n",
    "            'cross_validation_folds': 5,\n",
    "            'models_tested': list(all_results.keys())\n",
    "        },\n",
    "        'features': {\n",
    "            'numeric': numeric_features,\n",
    "            'categorical': categorical_features\n",
    "        },\n",
    "        'deliverables': [\n",
    "            best_model_filename,\n",
    "            comparison_filename,\n",
    "            processed_filename,\n",
    "            'output/plots/energy_distribution_improved.png',\n",
    "            'output/plots/feature_correlations_enhanced.png',\n",
    "            'output/model_comparison/model_comparison.png'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    report = convert_to_json_serializable(report)\n",
    "    report_filename = f'output/reports/final_report_comprehensive_{timestamp}.json'\n",
    "    with open(report_filename, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Comprehensive report saved: {report_filename}\")\n",
    "\n",
    "def print_final_summary_enhanced(engineered_df, y, numeric_features, \n",
    "                                categorical_features, all_results, \n",
    "                                best_model_name, execution_time):\n",
    "    \"\"\"Print final enhanced summary\"\"\"\n",
    "    best_model_metrics = all_results[best_model_name]['metrics']\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL ENHANCED RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ðŸ† Best Model: {best_model_name}\")\n",
    "    print(f\"ðŸ”¹ Model Performance:\")\n",
    "    print(f\"   â€¢ Test RÂ²: {best_model_metrics['test_r2']:.4f} ({best_model_metrics['test_r2']*100:.1f}% variance explained)\")\n",
    "    print(f\"   â€¢ CV Mean RÂ²: {best_model_metrics['cv_mean_r2']:.4f} (robust estimate)\")\n",
    "    print(f\"   â€¢ Test MAE: {best_model_metrics['test_mae']:,.0f} kWh/year\")\n",
    "    print(f\"   â€¢ Test MAPE: {best_model_metrics['test_mape']:.1f}% error\")\n",
    "    \n",
    "    print(f\"\\nðŸ”¹ Model Comparison (Test RÂ²):\")\n",
    "    for model_name, result in all_results.items():\n",
    "        indicator = \"ðŸ†\" if model_name == best_model_name else \"  \"\n",
    "        print(f\"   {indicator} {model_name:20s}: {result['metrics']['test_r2']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ”¹ Dataset Information:\")\n",
    "    print(f\"   â€¢ Apartments analyzed: {len(engineered_df):,}\")\n",
    "    print(f\"   â€¢ Features used: {len(numeric_features)} numeric, {len(categorical_features)} categorical\")\n",
    "    print(f\"   â€¢ Energy range: {y.min():,.0f} to {y.max():,.0f} kWh/year\")\n",
    "    print(f\"   â€¢ Average consumption: {y.mean():,.0f} kWh/year\")\n",
    "    \n",
    "    print(f\"\\nðŸ”¹ Enhanced Features:\")\n",
    "    print(f\"   â€¢ Improved realistic target distribution created\")\n",
    "    print(f\"   â€¢ Model comparison performed ({len(all_results)} models)\")\n",
    "    print(f\"   â€¢ Cross-validation with 5-fold\")\n",
    "    print(f\"   â€¢ Variance threshold filtering applied\")\n",
    "    print(f\"   â€¢ Memory optimization with explicit cleanup\")\n",
    "    \n",
    "    print(f\"\\nðŸ”¹ Swiss Context:\")\n",
    "    print(f\"   â€¢ Based on Swiss building characteristics\")\n",
    "    print(f\"   â€¢ Accounts for building age, layout, and regional factors\")\n",
    "    print(f\"   â€¢ Suitable for Swiss building stock energy analysis\")\n",
    "    \n",
    "    print(f\"\\nðŸ”¹ Execution Information:\")\n",
    "    print(f\"   â€¢ Total execution time: {execution_time:.1f} seconds\")\n",
    "    print(f\"   â€¢ Memory optimized pipeline\")\n",
    "    print(f\"   â€¢ All outputs saved to 'output/' directory\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    if best_model_metrics['test_r2'] > 0.7 and best_model_metrics['cv_mean_r2'] > 0.65:\n",
    "        print(\"ðŸŽ‰ EXCELLENT: Strong and robust predictive power!\")\n",
    "    elif best_model_metrics['test_r2'] > 0.5:\n",
    "        print(\"ðŸ‘ GOOD: Reliable predictions for Swiss dwellings\")\n",
    "    elif best_model_metrics['test_r2'] > 0.3:\n",
    "        print(\"âš ï¸  MODERATE: Some predictive value\")\n",
    "    else:\n",
    "        print(\"ðŸ”§ FAIR: Model provides baseline predictions\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "def save_error_report(e, traceback):\n",
    "    \"\"\"Save error report\"\"\"\n",
    "    error_report = {\n",
    "        'error': str(e),\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'traceback': traceback.format_exc()\n",
    "    }\n",
    "    \n",
    "    with open('output/reports/error_report_final.json', 'w') as f:\n",
    "        json.dump(convert_to_json_serializable(error_report), f, indent=2)\n",
    "    \n",
    "    print(f\"\\nError report saved to output/reports/error_report_final.json\")\n",
    "\n",
    "# ==============================================================================\n",
    "# RUN THE FINAL ENHANCED PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SWISS DWELLINGS - FINAL ENHANCED OPTIMIZED PIPELINE (COMPLETELY FIXED)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Completely fixed issues:\")\n",
    "    print(\"â€¢ Added y_full parameter to save_final_results function\")\n",
    "    print(\"â€¢ Improved realistic target distribution with mixture model\")\n",
    "    print(\"â€¢ Better error handling and basic output creation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = run_final_enhanced_analysis()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if results:\n",
    "        total_time = end_time - start_time\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"âœ… FINAL ENHANCED ANALYSIS COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total execution time: {total_time:.1f} seconds\")\n",
    "        print(f\"Best Model: {results['best_model_name']}\")\n",
    "        print(f\"Best Test RÂ²: {results['best_model']['metrics']['test_r2']:.4f}\")\n",
    "        print(f\"Best Test MAE: {results['best_model']['metrics']['test_mae']:,.0f} kWh/year\")\n",
    "        print(f\"Features used: {len(results['features']['numeric'])} numeric, {len(results['features']['categorical'])} categorical\")\n",
    "        print(f\"\\nAll outputs saved to 'output/' directory\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"âš ï¸  ANALYSIS COMPLETED WITH BASIC OUTPUTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Check output/reports/error_report_final.json for details\")\n",
    "        print(\"Basic outputs created in 'output/' directory\")\n",
    "        print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
